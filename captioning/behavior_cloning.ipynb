{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from piqa import SSIM\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from datasets.image_vae_dataset import ImageVAEDataset, ImageVAEDatasetFull\n",
    "from datasets.bc_dataset import BCDataset\n",
    "from models.visual_autoencoder import VisualAutoencoder\n",
    "from models.bc_policy import LanguageConditionedPolicy\n",
    "\n",
    "import config as CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training)\n",
    "train_dataset = ImageVAEDataset(dataset_path=CFG.datapath_training, caption_path=caption_path_training)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_val)\n",
    "val_dataset = ImageVAEDataset(dataset_path=CFG.datapath_val, caption_path=caption_path_val)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_training)\n",
    "train_loader_full = DataLoader(train_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_val)\n",
    "val_loader_full = DataLoader(val_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "for idx, batch in enumerate(train_dataset):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    plt.imshow(np.transpose(batch.img_static, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "\"\"\"\n",
    "for idx, batch in enumerate(train_loader_full):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 512\n",
    "learning_rate = 0.0001\n",
    "\n",
    "class SSIMLoss(SSIM):\n",
    "    def forward(self, x, y):\n",
    "        return 1. - super().forward(x, y)\n",
    "\n",
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Train the model\n",
    "    step = 0\n",
    "    checkpoint_step = 0;\n",
    "    #use tqdm for progress bar\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{CFG.epochs}\")\n",
    "        \n",
    "        #for i, data in enumerate(tqdm(train_loader)):\n",
    "        for i, data in enumerate(tqdm(train_loader_full)):\n",
    "            # Get the inputs and move them to the GPU if available\n",
    "            # inputs = data['img_static']\n",
    "            inputs = data['img_gripper']\n",
    "            inputs = inputs.to(CFG.device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            reconstruction_loss = criterion(outputs, inputs)\n",
    "            #ssim_loss = SSIM_criterion(outputs, inputs) * 0.001\n",
    "\n",
    "            loss = reconstruction_loss #+ ssim_loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({\"Total Loss\": loss.item()}, step=step)\n",
    "            wandb.log({\"Reconstruction Loss\": reconstruction_loss.item()}, step=step)\n",
    "            #wandb.log({\"SSIM Loss\": ssim_loss.item()}, step=step)\n",
    "\n",
    "            #if step % 321 == 0:\n",
    "            if step % 2000 == 0:\n",
    "                #visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu())\n",
    "                wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "                #save model weights\n",
    "                torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_gripper/image_vae_gripper_high_res-{checkpoint_step:03d}.pt\")\n",
    "                checkpoint_step += 1\n",
    "            \n",
    "            step += 1\n",
    "        \"\"\"\n",
    "        wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "        torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_static_long/image_vae_static_high_res-{epoch:03d}.pt\")\n",
    "        \"\"\"\n",
    "\n",
    "wandb.init(project=\"ImageVAE\")\n",
    "model = VisualAutoencoder(encoding_size)\n",
    "model.to(CFG.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "SSIM_criterion = SSIMLoss().cuda()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate image vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "def reconstruct(model):\n",
    "    for i, data in enumerate(val_loader_full):\n",
    "        inputs = data['img_static']\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        outputs = model(inputs)\n",
    "        for i in range(0, 6):\n",
    "            visualize_reconstruction(inputs[i].detach().cpu(), outputs[i].detach().cpu())\n",
    "        break\n",
    "    \n",
    "encoding_size = 512\n",
    "model = VisualAutoencoder(encoding_size).to(CFG.device)\n",
    "model.load_state_dict(torch.load('checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-045.pt', map_location=CFG.device))\n",
    "model = model.eval()\n",
    "reconstruct(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Cloning Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training)\n",
    "train_dataset_bc = BCDataset(data_path=CFG.datapath_training, caption_path=caption_path_training)\n",
    "train_loader_bc = DataLoader(train_dataset_bc, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_val)\n",
    "val_dataset_bc = BCDataset(data_path=CFG.datapath_val, caption_path=caption_path_val)\n",
    "val_loader_bc = DataLoader(val_dataset_bc,\n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataloader bc policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader_bc):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    print(\"action: \", batch.action.shape)\n",
    "    print(\"text_encoding: \", batch.text_encoding.shape)\n",
    "\n",
    "    print(\"img_static: \", batch.img_static.dtype)\n",
    "    print(\"img_gripper: \", batch.img_gripper.dtype)\n",
    "    print(\"action: \", batch.action.dtype)\n",
    "    print(\"text_encoding: \", batch.text_encoding.dtype)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BC Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc mean and std of robot actions (used in bc_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std for entire training data\n",
    "\n",
    "datafiles = os.listdir(CFG.datapath_training)\n",
    "\n",
    "all_actions = []\n",
    "# Loop through and print the file names\n",
    "for file in tqdm(datafiles):\n",
    "    try:\n",
    "        file_path = os.path.join(CFG.datapath_training, file)\n",
    "        data = np.load(file_path)\n",
    "        all_actions.append(data[\"actions\"])\n",
    "    except:\n",
    "        print(f\"Skipping '{file}'\")\n",
    "\n",
    "mean = np.mean(np.array(all_actions), axis=0)\n",
    "std = np.std(np.array(all_actions), axis=0)\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"std: \", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  [ 0.05829782 -0.11443839  0.5123094   0.97989569 -0.03639337  1.55281177\n",
      "  0.01424668]\n",
      "std:  [0.1525908  0.09533093 0.05158806 2.920689   0.10144497 0.56220544\n",
      " 0.99989851]\n"
     ]
    }
   ],
   "source": [
    "# mean and std only for captioned training data \n",
    "\n",
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training_parsed)\n",
    "annotations = np.load(f\"{caption_path_training}\", allow_pickle=True).item()\n",
    "annotations = annotations[\"info\"][\"indx\"]\n",
    "\n",
    "all_actions = []\n",
    "\n",
    "for range in annotations:\n",
    "    files = [f\"episode_{i:07d}.npz\" for i in range]\n",
    "    for file in files:\n",
    "        try:\n",
    "            file_path = os.path.join(CFG.datapath_training, file)\n",
    "            data = np.load(file_path)\n",
    "            all_actions.append(data[\"actions\"])\n",
    "        except:\n",
    "            print(f\"Skipping '{file}'\")\n",
    "\n",
    "mean = np.mean(np.array(all_actions), axis=0)\n",
    "std = np.std(np.array(all_actions), axis=0)\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fgo6xqf8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-water-33</strong> at: <a href='https://wandb.ai/timlauffs/skillGPT_BC/runs/fgo6xqf8' target=\"_blank\">https://wandb.ai/timlauffs/skillGPT_BC/runs/fgo6xqf8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230921_013136-fgo6xqf8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fgo6xqf8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/tim/E/hulc_captioning/captioning/wandb/run-20230921_013212-lpj6ygff</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timlauffs/skillGPT_BC/runs/lpj6ygff' target=\"_blank\">charmed-tree-34</a></strong> to <a href='https://wandb.ai/timlauffs/skillGPT_BC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timlauffs/skillGPT_BC' target=\"_blank\">https://wandb.ai/timlauffs/skillGPT_BC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timlauffs/skillGPT_BC/runs/lpj6ygff' target=\"_blank\">https://wandb.ai/timlauffs/skillGPT_BC/runs/lpj6ygff</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[39m# Save the model checkpoint\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         torch\u001b[39m.\u001b[39msave(policy\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./checkpoints/bc_policy/bc_policy_gripper/bc_test_policy_static-\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m train()\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(num_epochs):\n\u001b[1;32m     38\u001b[0m     \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     policy\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m     epoch_train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"skillGPT_BC\")\n",
    "device = CFG.device\n",
    "\n",
    "#hyperparameters\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "language_dim = 512\n",
    "image_dim = 512\n",
    "action_dim = 7\n",
    "\n",
    "def train():\n",
    "    # loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize the model\n",
    "    policy = LanguageConditionedPolicy(language_dim, action_dim)\n",
    "    policy = policy.to(device)\n",
    "\n",
    "    # initialize the visual autoencoder\n",
    "    visual_autoencoder = VisualAutoencoder(512)\n",
    "    visual_autoencoder = visual_autoencoder.to(device)\n",
    "\n",
    "    # load the weights\n",
    "   #  visual_autoencoder.load_state_dict(torch.load(\"./checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-045.pt\"))\n",
    "    visual_autoencoder.load_state_dict(torch.load(\"./checkpoints/image_vae/image_vae_gripper/image_vae_gripper_high_res-017.pt\"))\n",
    "    image_encoder = visual_autoencoder.encoder\n",
    "    for param in image_encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Define the optimizer\n",
    "    params = list(policy.parameters()) + list(image_encoder.parameters())\n",
    "    optimizer = optim.Adam(params, lr=lr)\n",
    "\n",
    "    step = 0\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        policy.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for i, data in enumerate(tqdm(train_loader_bc)):\n",
    "\n",
    "            language_emb = data[\"text_encoding\"].to(device).to(torch.float)\n",
    "          #   image = data[\"img_static\"].to(device)\n",
    "            image = data[\"img_gripper\"].to(device)\n",
    "            action = data[\"action\"].to(device)\n",
    "            #action = data[\"action\"].to(torch.float32).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            image_features = image_encoder(image)\n",
    "            pred_action = policy(language_emb, image_features)\n",
    "            loss = criterion(pred_action, action)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            wandb.log({\"train_loss\": loss.item()}, step)\n",
    "            step += 1\n",
    "            \n",
    "        # Print the average training loss for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / len(train_loader_bc)\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        policy.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(val_loader_bc)):\n",
    "                language_emb = data[\"text_encoding\"].to(device).to(torch.float)\n",
    "             #   image = data[\"img_static\"].to(device)\n",
    "                image = data[\"img_gripper\"].to(device)\n",
    "                action = data[\"action\"].to(device)\n",
    "                image_features = image_encoder(image)\n",
    "                pred_action = policy(language_emb, image_features)\n",
    "                loss = criterion(pred_action, action)\n",
    "                epoch_val_loss += loss.item()\n",
    "                \n",
    "        # Print the average validation loss for the epoch\n",
    "        avg_epoch_val_loss = epoch_val_loss / len(val_loader_bc)\n",
    "        print(f\"Epoch {epoch+1} Validation Loss: {avg_epoch_val_loss:.8f}\")\n",
    "        wandb.log({\"val_loss\": avg_epoch_val_loss}, step)\n",
    "        \n",
    "        # Save the model checkpoint\n",
    "        torch.save(policy.state_dict(), f\"./checkpoints/bc_policy/bc_policy_gripper/bc_test_policy_static-{epoch:03d}.pt\")\n",
    "        \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
