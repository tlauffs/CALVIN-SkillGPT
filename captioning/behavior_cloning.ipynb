{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from piqa import SSIM\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from datasets.image_vae_dataset import ImageVAEDataset, ImageVAEDatasetFull\n",
    "from datasets.bc_dataset import BCDataset\n",
    "from models.visual_autoencoder import VisualAutoencoder\n",
    "from models.bc_policy import LanguageConditionedPolicy\n",
    "import config as CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training)\n",
    "train_dataset = ImageVAEDataset(dataset_path=CFG.datapath_training, caption_path=caption_path_training)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_val)\n",
    "val_dataset = ImageVAEDataset(dataset_path=CFG.datapath_val, caption_path=caption_path_val)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_training)\n",
    "train_loader_full = DataLoader(train_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_val)\n",
    "val_loader_full = DataLoader(val_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n",
    "\n",
    "for idx, batch in enumerate(train_loader_full):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 512\n",
    "learning_rate = 0.0001\n",
    "\n",
    "class SSIMLoss(SSIM):\n",
    "    def forward(self, x, y):\n",
    "        return 1. - super().forward(x, y)\n",
    "\n",
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Train the model\n",
    "    step = 0\n",
    "    checkpoint_step = 0;\n",
    "    #use tqdm for progress bar\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{CFG.epochs}\")\n",
    "        \n",
    "        #for i, data in enumerate(tqdm(train_loader)):\n",
    "        for i, data in enumerate(tqdm(train_loader_full)):\n",
    "            # Get the inputs and move them to the GPU if available\n",
    "            inputs = data['img_static']\n",
    "            inputs = inputs.to(CFG.device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            reconstruction_loss = criterion(outputs, inputs)\n",
    "            #ssim_loss = SSIM_criterion(outputs, inputs) * 0.001\n",
    "\n",
    "            loss = reconstruction_loss #+ ssim_loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({\"Total Loss\": loss.item()}, step=step)\n",
    "            wandb.log({\"Reconstruction Loss\": reconstruction_loss.item()}, step=step)\n",
    "            #wandb.log({\"SSIM Loss\": ssim_loss.item()}, step=step)\n",
    "\n",
    "            #if step % 321 == 0:\n",
    "            if step % 2000 == 0:\n",
    "                #visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu())\n",
    "                wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "                #save model weights\n",
    "                torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-{checkpoint_step:03d}.pt\")\n",
    "                checkpoint_step += 1\n",
    "            \n",
    "            step += 1\n",
    "        \"\"\"\n",
    "        wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "        torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_static_long/image_vae_static_high_res-{epoch:03d}.pt\")\n",
    "        \"\"\"\n",
    "\n",
    "wandb.init(project=\"ImageVAE\")\n",
    "model = VisualAutoencoder(encoding_size)\n",
    "model.to(CFG.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "SSIM_criterion = SSIMLoss().cuda()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate image vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "def reconstruct(model):\n",
    "    for i, data in enumerate(val_loader_full):\n",
    "        inputs = data['img_static']\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        outputs = model(inputs)\n",
    "        for i in range(0, 6):\n",
    "            visualize_reconstruction(inputs[i].detach().cpu(), outputs[i].detach().cpu())\n",
    "        break\n",
    "    \n",
    "encoding_size = 512\n",
    "model = VisualAutoencoder(encoding_size).to(CFG.device)\n",
    "model.load_state_dict(torch.load('checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-045.pt', map_location=CFG.device))\n",
    "model = model.eval()\n",
    "reconstruct(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Cloning Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training)\n",
    "train_dataset_bc = BCDataset(data_path=CFG.datapath_training, caption_path=caption_path_training)\n",
    "train_loader_bc = DataLoader(train_dataset_bc, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_val)\n",
    "val_dataset_bc = BCDataset(data_path=CFG.datapath_val, caption_path=caption_path_val)\n",
    "val_loader_bc = DataLoader(val_dataset_bc,\n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataloader bc policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader_bc):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    print(\"action: \", batch.action.shape)\n",
    "    print(\"text_encoding: \", batch.text_encoding.shape)\n",
    "\n",
    "    print(\"img_static: \", batch.img_static.dtype)\n",
    "    print(\"img_gripper: \", batch.img_gripper.dtype)\n",
    "    print(\"action: \", batch.action.dtype)\n",
    "    print(\"text_encoding: \", batch.text_encoding.dtype)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BC Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc mean and std of robot actions (used in bc_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18916/512093 [00:03<01:27, 5614.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_huggingface_distilroberta'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 29331/512093 [00:05<01:25, 5621.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_all-mpnet-base-v2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 62716/512093 [00:11<01:20, 5590.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_clip_ViTB32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 73362/512093 [00:13<01:18, 5562.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'scene_info.npy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 118466/512093 [00:22<01:15, 5214.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_clip_resnet50'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 225185/512093 [00:41<00:53, 5411.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_BERT'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 252485/512093 [00:46<00:47, 5495.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_annotations'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 254713/512093 [00:47<00:46, 5563.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_all-distilroberta-v1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 256408/512093 [00:47<00:45, 5617.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_huggingface_mpnet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 305779/512093 [00:56<00:37, 5572.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_msmarco-bert-base-dot-v5'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 320296/512093 [00:58<00:34, 5597.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_all-MiniLM-L6-v2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 377467/512093 [01:09<00:25, 5321.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'lang_paraphrase-MiniLM-L3-v2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 424024/512093 [01:18<00:17, 5120.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'ep_start_end_ids.npy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 458941/512093 [01:24<00:09, 5538.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'ep_lens.npy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 480045/512093 [01:28<00:05, 5475.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping '.hydra'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 505671/512093 [01:33<00:01, 5470.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'statistics.yaml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512093/512093 [01:34<00:00, 5423.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  [ 0.05244775 -0.12080607  0.50815218  1.01496132 -0.03902264  1.56418701\n",
      "  0.13438409]\n",
      "std:  [0.15992226 0.10983621 0.0623301  2.90982278 0.10183952 0.34633791\n",
      " 0.99092932]\n"
     ]
    }
   ],
   "source": [
    "# mean and std for entire training data\n",
    "\n",
    "datafiles = os.listdir(CFG.datapath_training)\n",
    "\n",
    "all_actions = []\n",
    "# Loop through and print the file names\n",
    "for file in tqdm(datafiles):\n",
    "    try:\n",
    "        file_path = os.path.join(CFG.datapath_training, file)\n",
    "        data = np.load(file_path)\n",
    "        all_actions.append(data[\"actions\"])\n",
    "    except:\n",
    "        print(f\"Skipping '{file}'\")\n",
    "\n",
    "mean = np.mean(np.array(all_actions), axis=0)\n",
    "std = np.std(np.array(all_actions), axis=0)\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"std: \", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  [ 0.05256253 -0.12068113  0.50823375  1.01427333 -0.03897106  1.56396382\n",
      "  0.13202699]\n",
      "std:  [0.15978371 0.10957365 0.06213988 2.91004043 0.10183245 0.35185281\n",
      " 0.99124612]\n"
     ]
    }
   ],
   "source": [
    "# mean and std only for captioned training data \n",
    "\n",
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training_parsed)\n",
    "annotations = np.load(f\"{caption_path_training}\", allow_pickle=True).item()\n",
    "annotations = annotations[\"info\"][\"indx\"]\n",
    "\n",
    "for range in annotations:\n",
    "    files = [f\"episode_{i:07d}.npz\" for i in range]\n",
    "    for file in files:\n",
    "        try:\n",
    "            file_path = os.path.join(CFG.datapath_training, file)\n",
    "            data = np.load(file_path)\n",
    "            all_actions.append(data[\"actions\"])\n",
    "        except:\n",
    "            print(f\"Skipping '{file}'\")\n",
    "\n",
    "mean = np.mean(np.array(all_actions), axis=0)\n",
    "std = np.std(np.array(all_actions), axis=0)\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimlauffs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/tim/E/hulc_captioning/captioning/wandb/run-20230917_005732-2biuxx9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timlauffs/skillGPT_BC/runs/2biuxx9f' target=\"_blank\">zany-frog-13</a></strong> to <a href='https://wandb.ai/timlauffs/skillGPT_BC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timlauffs/skillGPT_BC' target=\"_blank\">https://wandb.ai/timlauffs/skillGPT_BC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timlauffs/skillGPT_BC/runs/2biuxx9f' target=\"_blank\">https://wandb.ai/timlauffs/skillGPT_BC/runs/2biuxx9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/321 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BCDataset' object has no attribute 'action_stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[39m# Save the model checkpoint\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         torch\u001b[39m.\u001b[39msave(policy\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./checkpoints/bc_policy/bc_policy_static_2/bc_policy_static-\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m train()\n",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m policy\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     39\u001b[0m epoch_train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_loader_bc)):\n\u001b[1;32m     42\u001b[0m     language_emb \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtext_encoding\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     43\u001b[0m     image \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mimg_gripper\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/media/tim/E/hulc_captioning/captioning/datasets/bc_dataset.py:75\u001b[0m, in \u001b[0;36mBCDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     73\u001b[0m action \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mactions\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[39m# normalize action\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m action \u001b[39m=\u001b[39m ((action \u001b[39m-\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_stats\u001b[39m.\u001b[39mmean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_stats\u001b[39m.\u001b[39mstd)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     76\u001b[0m img_static \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img_static)\n\u001b[1;32m     77\u001b[0m img_gripper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img_gripper)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BCDataset' object has no attribute 'action_stats'"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"skillGPT_BC\")\n",
    "device = CFG.device\n",
    "\n",
    "#hyperparameters\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "language_dim = 512\n",
    "image_dim = 512\n",
    "action_dim = 7\n",
    "\n",
    "def train():\n",
    "    # loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize the model\n",
    "    policy = LanguageConditionedPolicy(language_dim, action_dim)\n",
    "    policy = policy.to(device)\n",
    "\n",
    "    # initialize the visual autoencoder\n",
    "    visual_autoencoder = VisualAutoencoder(512)\n",
    "    visual_autoencoder = visual_autoencoder.to(device)\n",
    "\n",
    "    # load the weights\n",
    "    visual_autoencoder.load_state_dict(torch.load(\"./checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-045.pt\"))\n",
    "    image_encoder = visual_autoencoder.encoder\n",
    "    for param in image_encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Define the optimizer\n",
    "    params = list(policy.parameters()) + list(image_encoder.parameters())\n",
    "    optimizer = optim.Adam(params, lr=lr)\n",
    "\n",
    "    step = 0\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        policy.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for i, data in enumerate(tqdm(train_loader_bc)):\n",
    "\n",
    "            language_emb = data[\"text_encoding\"].to(device).to(torch.float)\n",
    "            image = data[\"img_gripper\"].to(device)\n",
    "            action = data[\"action\"].to(device)\n",
    "            #action = data[\"action\"].to(torch.float32).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            image_features = image_encoder(image)\n",
    "            pred_action = policy(language_emb, image_features)\n",
    "            loss = criterion(pred_action, action)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            wandb.log({\"train_loss\": loss.item()}, step)\n",
    "            step += 1\n",
    "            \n",
    "        # Print the average training loss for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / len(train_loader_bc)\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        policy.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(val_loader_bc)):\n",
    "                language_emb = data[\"text_encoding\"].to(device).to(torch.float)\n",
    "                image = data[\"img_gripper\"].to(device)\n",
    "                action = data[\"action\"].to(device)\n",
    "                image_features = image_encoder(image)\n",
    "                pred_action = policy(language_emb, image_features)\n",
    "                loss = criterion(pred_action, action)\n",
    "                epoch_val_loss += loss.item()\n",
    "                \n",
    "        # Print the average validation loss for the epoch\n",
    "        avg_epoch_val_loss = epoch_val_loss / len(val_loader_bc)\n",
    "        print(f\"Epoch {epoch+1} Validation Loss: {avg_epoch_val_loss:.8f}\")\n",
    "        wandb.log({\"val_loss\": avg_epoch_val_loss}, step)\n",
    "        \n",
    "        # Save the model checkpoint\n",
    "        torch.save(policy.state_dict(), f\"./checkpoints/bc_policy/bc_policy_static_2/bc_policy_static-{epoch:03d}\")\n",
    "        \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
