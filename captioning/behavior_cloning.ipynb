{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from piqa import SSIM\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from datasets.image_vae_dataset import ImageVAEDataset, ImageVAEDatasetFull\n",
    "from models.visual_autoencoder import VisualAutoencoder\n",
    "import config as CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training)\n",
    "train_dataset = ImageVAEDataset(dataset_path=CFG.datapath_training, caption_path=caption_path_training)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_val)\n",
    "val_dataset = ImageVAEDataset(dataset_path=CFG.datapath_val, caption_path=caption_path_val)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_training)\n",
    "train_loader_full = DataLoader(train_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_val)\n",
    "val_loader_full = DataLoader(val_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n",
    "\n",
    "for idx, batch in enumerate(train_loader_full):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 512\n",
    "learning_rate = 0.0001\n",
    "\n",
    "class SSIMLoss(SSIM):\n",
    "    def forward(self, x, y):\n",
    "        return 1. - super().forward(x, y)\n",
    "\n",
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Train the model\n",
    "    step = 0\n",
    "    checkpoint_step = 0;\n",
    "    #use tqdm for progress bar\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{CFG.epochs}\")\n",
    "        \n",
    "        #for i, data in enumerate(tqdm(train_loader)):\n",
    "        for i, data in enumerate(tqdm(train_loader_full)):\n",
    "            # Get the inputs and move them to the GPU if available\n",
    "            inputs = data['img_static']\n",
    "            inputs = inputs.to(CFG.device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            reconstruction_loss = criterion(outputs, inputs)\n",
    "            #ssim_loss = SSIM_criterion(outputs, inputs) * 0.001\n",
    "\n",
    "            loss = reconstruction_loss #+ ssim_loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({\"Total Loss\": loss.item()}, step=step)\n",
    "            wandb.log({\"Reconstruction Loss\": reconstruction_loss.item()}, step=step)\n",
    "            #wandb.log({\"SSIM Loss\": ssim_loss.item()}, step=step)\n",
    "\n",
    "            #if step % 321 == 0:\n",
    "            if step % 2000 == 0:\n",
    "                #visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu())\n",
    "                wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "                #save model weights\n",
    "                torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-{checkpoint_step:03d}.pt\")\n",
    "                checkpoint_step += 1\n",
    "            \n",
    "            step += 1\n",
    "        \"\"\"\n",
    "        wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "        torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_static_long/image_vae_static_high_res-{epoch:03d}.pt\")\n",
    "        \"\"\"\n",
    "\n",
    "wandb.init(project=\"ImageVAE\")\n",
    "model = VisualAutoencoder(encoding_size)\n",
    "model.to(CFG.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "SSIM_criterion = SSIMLoss().cuda()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate image vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "def reconstruct(model):\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs = data['img_static']\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        outputs = model(inputs)\n",
    "        for i in range(0, 15):\n",
    "            visualize_reconstruction(inputs[i].detach().cpu(), outputs[i].detach().cpu())\n",
    "        break\n",
    "    \n",
    "encoding_size = 512\n",
    "model = VisualAutoencoder(encoding_size).to(CFG.device)\n",
    "model.load_state_dict(torch.load('checkpoints/image_vae/image_vae_static/image_vae_static_high_res-039.pt', map_location=CFG.device))\n",
    "model = model.eval()\n",
    "reconstruct(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
