{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from piqa import SSIM\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from datasets.image_vae_dataset import ImageVAEDataset, ImageVAEDatasetFull\n",
    "from datasets.bc_dataset import BCDataset\n",
    "from models.visual_autoencoder import VisualAutoencoder\n",
    "from models.bc_policy import LanguageConditionedPolicy\n",
    "\n",
    "import config as CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training)\n",
    "train_dataset = ImageVAEDataset(dataset_path=CFG.datapath_training, caption_path=caption_path_training)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_val)\n",
    "val_dataset = ImageVAEDataset(dataset_path=CFG.datapath_val, caption_path=caption_path_val)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_training)\n",
    "train_loader_full = DataLoader(train_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset_full = ImageVAEDatasetFull(dataset_path=CFG.datapath_val)\n",
    "val_loader_full = DataLoader(val_dataset_full,\n",
    "                          #num_workers=16,\n",
    "                          #prefetch_factor=6, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "for idx, batch in enumerate(train_dataset):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    plt.imshow(np.transpose(batch.img_static, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "\"\"\"\n",
    "for idx, batch in enumerate(train_loader_full):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 512\n",
    "learning_rate = 0.0001\n",
    "\n",
    "class SSIMLoss(SSIM):\n",
    "    def forward(self, x, y):\n",
    "        return 1. - super().forward(x, y)\n",
    "\n",
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Train the model\n",
    "    step = 0\n",
    "    checkpoint_step = 0;\n",
    "    #use tqdm for progress bar\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{CFG.epochs}\")\n",
    "        \n",
    "        #for i, data in enumerate(tqdm(train_loader)):\n",
    "        for i, data in enumerate(tqdm(train_loader_full)):\n",
    "            # Get the inputs and move them to the GPU if available\n",
    "            # inputs = data['img_static']\n",
    "            inputs = data['img_gripper']\n",
    "            inputs = inputs.to(CFG.device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            reconstruction_loss = criterion(outputs, inputs)\n",
    "            #ssim_loss = SSIM_criterion(outputs, inputs) * 0.001\n",
    "\n",
    "            loss = reconstruction_loss #+ ssim_loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({\"Total Loss\": loss.item()}, step=step)\n",
    "            wandb.log({\"Reconstruction Loss\": reconstruction_loss.item()}, step=step)\n",
    "            #wandb.log({\"SSIM Loss\": ssim_loss.item()}, step=step)\n",
    "\n",
    "            #if step % 321 == 0:\n",
    "            if step % 2000 == 0:\n",
    "                #visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu())\n",
    "                wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "                #save model weights\n",
    "                torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_gripper/image_vae_gripper_high_res-{checkpoint_step:03d}.pt\")\n",
    "                checkpoint_step += 1\n",
    "            \n",
    "            step += 1\n",
    "        \"\"\"\n",
    "        wandb.log({\"Training Images\": wandb.Image(visualize_reconstruction(inputs[0].detach().cpu(), outputs[0].detach().cpu()))}, step=step)\n",
    "        torch.save(model.state_dict(), f\"checkpoints/image_vae/image_vae_static_long/image_vae_static_high_res-{epoch:03d}.pt\")\n",
    "        \"\"\"\n",
    "\n",
    "wandb.init(project=\"ImageVAE\")\n",
    "model = VisualAutoencoder(encoding_size)\n",
    "model.to(CFG.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "SSIM_criterion = SSIMLoss().cuda()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate image vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(original, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(original.permute(1, 2, 0))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(reconstructed.detach().permute(1, 2, 0))\n",
    "    axs[1].set_title('Reconstructed')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # return figure for wandb\n",
    "    return fig\n",
    "\n",
    "def reconstruct(model):\n",
    "    for i, data in enumerate(val_loader_full):\n",
    "        inputs = data['img_static']\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        outputs = model(inputs)\n",
    "        for i in range(0, 6):\n",
    "            visualize_reconstruction(inputs[i].detach().cpu(), outputs[i].detach().cpu())\n",
    "        break\n",
    "    \n",
    "encoding_size = 512\n",
    "model = VisualAutoencoder(encoding_size).to(CFG.device)\n",
    "model.load_state_dict(torch.load('checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-045.pt', map_location=CFG.device))\n",
    "model = model.eval()\n",
    "reconstruct(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Cloning Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_training)\n",
    "train_dataset_bc = BCDataset(data_path=CFG.datapath_training, caption_path=caption_path_training)\n",
    "train_loader_bc = DataLoader(train_dataset_bc, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(CFG.datapath_val)\n",
    "val_dataset_bc = BCDataset(data_path=CFG.datapath_val, caption_path=caption_path_val)\n",
    "val_loader_bc = DataLoader(val_dataset_bc,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataloader bc policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_static:  torch.Size([32, 3, 180, 180])\n",
      "img_gripper:  torch.Size([32, 3, 180, 180])\n",
      "robot_obs:  torch.Size([32, 15])\n",
      "robot_obs:  tensor([ 0.1724, -0.3310,  0.5218, -3.1360, -0.1580,  1.3245,  0.0800, -1.2581,\n",
      "         1.1439,  1.8302, -2.1316, -1.0079,  1.7368,  0.3788,  1.0000])\n",
      "action:  torch.Size([32, 7])\n",
      "rel_action:  torch.Size([32, 7])\n",
      "text_encoding:  torch.Size([32, 512])\n",
      "img_static:  torch.float32\n",
      "img_gripper:  torch.float32\n",
      "action:  torch.float32\n",
      "rel_action:  torch.float32\n",
      "robot_obs:  torch.float32\n",
      "text_encoding:  torch.float16\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_loader_bc):\n",
    "    print(\"img_static: \", batch.img_static.shape)\n",
    "    print(\"img_gripper: \", batch.img_gripper.shape)\n",
    "    print(\"robot_obs: \", batch.robot_obs.shape)\n",
    "    print(\"robot_obs: \", batch.robot_obs[0])\n",
    "    print(\"action: \", batch.action.shape)\n",
    "    print(\"rel_action: \", batch.rel_action.shape)\n",
    "    print(\"text_encoding: \", batch.text_encoding.shape)\n",
    "\n",
    "    print(\"img_static: \", batch.img_static.dtype)\n",
    "    print(\"img_gripper: \", batch.img_gripper.dtype)\n",
    "    print(\"action: \", batch.action.dtype)\n",
    "    print(\"rel_action: \", batch.rel_action.dtype)\n",
    "    print(\"robot_obs: \", batch.robot_obs.dtype)\n",
    "    print(\"text_encoding: \", batch.text_encoding.dtype)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BC Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc mean and std of robot actions (used in bc_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std for entire training data\n",
    "\n",
    "datafiles = os.listdir(CFG.datapath_training)\n",
    "\n",
    "all_actions = []\n",
    "# Loop through and print the file names\n",
    "for file in tqdm(datafiles):\n",
    "    try:\n",
    "        file_path = os.path.join(CFG.datapath_training, file)\n",
    "        data = np.load(file_path)\n",
    "        all_actions.append(data[\"actions\"])\n",
    "    except:\n",
    "        print(f\"Skipping '{file}'\")\n",
    "\n",
    "mean = np.mean(np.array(all_actions), axis=0)\n",
    "std = np.std(np.array(all_actions), axis=0)\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"std: \", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimlauffs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/tim/E/hulc_captioning/captioning/wandb/run-20230921_045922-sfb9p1qj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/timlauffs/skillGPT_BC/runs/sfb9p1qj' target=\"_blank\">leafy-sponge-66</a></strong> to <a href='https://wandb.ai/timlauffs/skillGPT_BC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/timlauffs/skillGPT_BC' target=\"_blank\">https://wandb.ai/timlauffs/skillGPT_BC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/timlauffs/skillGPT_BC/runs/sfb9p1qj' target=\"_blank\">https://wandb.ai/timlauffs/skillGPT_BC/runs/sfb9p1qj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:46<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:07<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.06364228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:43<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:07<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 0.06202749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:44<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.0557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:08<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 0.05844997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 35/161 [00:09<00:35,  3.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# Save the model checkpoint\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         torch\u001b[39m.\u001b[39msave(policy\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./checkpoints/bc_policy/bc_rel/bc_policy-\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m train()\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m policy\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     45\u001b[0m epoch_train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_loader_bc)):\n\u001b[1;32m     48\u001b[0m     language_emb \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtext_encoding\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     49\u001b[0m   \u001b[39m#   image = data[\"img_static\"].to(device)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39m# action = data[\"action\"].to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/media/tim/E/hulc_captioning/captioning/datasets/bc_dataset.py:67\u001b[0m, in \u001b[0;36mBCDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# normalize action\u001b[39;00m\n\u001b[1;32m     66\u001b[0m action \u001b[39m=\u001b[39m ((action \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_stats\u001b[39m.\u001b[39mmean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_stats\u001b[39m.\u001b[39mstd)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> 67\u001b[0m img_static \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img_static)\n\u001b[1;32m     68\u001b[0m img_gripper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img_gripper)\n\u001b[1;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m AttrDict(img_static\u001b[39m=\u001b[39mimg_static,\n\u001b[1;32m     71\u001b[0m                 img_gripper\u001b[39m=\u001b[39mimg_gripper,\n\u001b[1;32m     72\u001b[0m                 robot_obs\u001b[39m=\u001b[39mrobot_obs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 text_encoding\u001b[39m=\u001b[39mclip_text_features\n\u001b[1;32m     76\u001b[0m                 )\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torchvision/transforms/functional.py:337\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput type \u001b[39m\u001b[39m{\u001b[39;00mnpimg\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m \u001b[39mreturn\u001b[39;00m Image\u001b[39m.\u001b[39;49mfromarray(npimg, mode\u001b[39m=\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/PIL/Image.py:3093\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3090\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mtostring()\n\u001b[0;32m-> 3093\u001b[0m \u001b[39mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m, rawmode, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/PIL/Image.py:3009\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3006\u001b[0m         im\u001b[39m.\u001b[39mreadonly \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   3007\u001b[0m         \u001b[39mreturn\u001b[39;00m im\n\u001b[0;32m-> 3009\u001b[0m \u001b[39mreturn\u001b[39;00m frombytes(mode, size, data, decoder_name, args)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/PIL/Image.py:2951\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2948\u001b[0m     args \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2950\u001b[0m im \u001b[39m=\u001b[39m new(mode, size)\n\u001b[0;32m-> 2951\u001b[0m im\u001b[39m.\u001b[39;49mfrombytes(data, decoder_name, args)\n\u001b[1;32m   2952\u001b[0m \u001b[39mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/PIL/Image.py:798\u001b[0m, in \u001b[0;36mImage.frombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    795\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[1;32m    797\u001b[0m \u001b[39m# unpack data\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m d \u001b[39m=\u001b[39m _getdecoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode, decoder_name, args)\n\u001b[1;32m    799\u001b[0m d\u001b[39m.\u001b[39msetimage(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mim)\n\u001b[1;32m    800\u001b[0m s \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mdecode(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/PIL/Image.py:392\u001b[0m, in \u001b[0;36m_getdecoder\u001b[0;34m(mode, decoder_name, args, extra)\u001b[0m\n\u001b[1;32m    390\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdecoder \u001b[39m\u001b[39m{\u001b[39;00mdecoder_name\u001b[39m}\u001b[39;00m\u001b[39m not available\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m \u001b[39mreturn\u001b[39;00m decoder(mode, \u001b[39m*\u001b[39;49margs \u001b[39m+\u001b[39;49m extra)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"skillGPT_BC\")\n",
    "device = CFG.device\n",
    "\n",
    "#hyperparameters\n",
    "# lr = 0.0001\n",
    "lr = 0.0001\n",
    "num_epochs = 1000\n",
    "language_dim = 512\n",
    "image_dim = 512\n",
    "action_dim = 7\n",
    "\n",
    "def train():\n",
    "    # loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize the model\n",
    "    policy = LanguageConditionedPolicy(language_dim, action_dim)\n",
    "    policy = policy.to(device)\n",
    "\n",
    "    # initialize the visual autoencoder\n",
    "    visual_autoencoder_static = VisualAutoencoder(512)\n",
    "    visual_autoencoder_static = visual_autoencoder_static.to(device)\n",
    "    visual_autoencoder_static.load_state_dict(torch.load(\"./checkpoints/image_vae/image_vae_static_full_long/image_vae_static_full_high_res-045.pt\"))\n",
    "    image_encoder_static = visual_autoencoder_static.encoder\n",
    "    for param in image_encoder_static.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    visual_autoencoder_gripper = VisualAutoencoder(512)\n",
    "    visual_autoencoder_gripper = visual_autoencoder_gripper.to(device)\n",
    "    visual_autoencoder_gripper.load_state_dict(torch.load(\"./checkpoints/image_vae/image_vae_gripper/image_vae_gripper_high_res-017.pt\"))\n",
    "    image_encoder_gripper = visual_autoencoder_gripper.encoder\n",
    "    for param in image_encoder_gripper.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    # Define the optimizer\n",
    "    params = list(policy.parameters()) + list(image_encoder_static.parameters()) + list(image_encoder_gripper.parameters()) \n",
    "    optimizer = optim.Adam(params, lr=lr)\n",
    "\n",
    "    step = 0\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        policy.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for i, data in enumerate(tqdm(train_loader_bc)):\n",
    "\n",
    "            language_emb = data[\"text_encoding\"].to(device).to(torch.float)\n",
    "          #   image = data[\"img_static\"].to(device)\n",
    "            action = data[\"action\"].to(device)\n",
    "            #action = data['rel_action'].to(device)\n",
    "\n",
    "            robot_obs = data[\"robot_obs\"].to(device)\n",
    "\n",
    "            image_static = data[\"img_static\"].to(device)\n",
    "            optimizer.zero_grad()    \n",
    "            image_features_static = image_encoder_static(image_static)\n",
    "\n",
    "            image_gripper = data[\"img_gripper\"].to(device)\n",
    "            optimizer.zero_grad()    \n",
    "            image_features_gripper = image_encoder_gripper(image_gripper)\n",
    "\n",
    "\n",
    "            pred_action = policy(language_emb, image_features_static, image_features_gripper, robot_obs)\n",
    "            loss = criterion(pred_action, action)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            wandb.log({\"train_loss\": loss.item()}, step)\n",
    "            step += 1\n",
    "            \n",
    "        # Print the average training loss for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / len(train_loader_bc)\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        policy.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(val_loader_bc)):\n",
    "                language_emb = data[\"text_encoding\"].to(device).to(torch.float)\n",
    "                image_static = data[\"img_static\"].to(device)\n",
    "                image_gripper = data[\"img_gripper\"].to(device)\n",
    "                action = data[\"action\"].to(device)\n",
    "                #action = data[\"rel_action\"].to(device)\n",
    "                robot_obs = data[\"robot_obs\"].to(device) \n",
    "\n",
    "                image_features_static = image_encoder_static(image_static)\n",
    "                image_features_gripper = image_encoder_gripper(image_gripper)\n",
    "\n",
    "\n",
    "                pred_action = policy(language_emb, image_features_static, image_features_gripper, robot_obs)\n",
    "                loss = criterion(pred_action, action)\n",
    "                epoch_val_loss += loss.item()\n",
    "                \n",
    "        # Print the average validation loss for the epoch\n",
    "        avg_epoch_val_loss = epoch_val_loss / len(val_loader_bc)\n",
    "        print(f\"Epoch {epoch+1} Validation Loss: {avg_epoch_val_loss:.8f}\")\n",
    "        wandb.log({\"val_loss\": avg_epoch_val_loss}, step)\n",
    "        \n",
    "        # Save the model checkpoint\n",
    "        torch.save(policy.state_dict(), f\"./checkpoints/bc_policy/bc_static_gripper_obs/bc_policy-{epoch:03d}.pt\")\n",
    "        \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
