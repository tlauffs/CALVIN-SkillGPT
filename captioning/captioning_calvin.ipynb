{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-07 00:01:29.711696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from r3m import load_r3m\n",
    "import clip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage, InterpolationMode\n",
    "\n",
    "from torch.nn import functional as nnf\n",
    "import wandb\n",
    "\n",
    "from dataset import CustomDataset, AttrDict \n",
    "from utils.visualize import visualize\n",
    "from models.caption_model import ClipCaptionModel, MappingType\n",
    "\n",
    "import config as CFG\n",
    "\n",
    "\n",
    "mp.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Calvin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath_test= '.././test_data/D_D/task_D_D_episode.npz'\n",
    "data = np.load(datapath_test)\n",
    "print(list(data.keys()))\n",
    "print(data['actions'].shape)\n",
    "print(data['rel_actions'].shape)\n",
    "print(data['rgb_static'].shape)\n",
    "print(data['rgb_gripper'].shape)\n",
    "print(data['scene_obs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find sutiable caption for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = CFG.datapath_training_parsed + \"/lang_annotations/auto_lang_ann.npy\"\n",
    "\n",
    "annotations = np.load(path, allow_pickle=True).item()\n",
    "annotations = annotations[\"language\"][\"ann\"]\n",
    "\n",
    "\n",
    "unique, counts = np.unique(annotations, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "\"\"\"\n",
    "unique_annotaions = set(annotations)\n",
    "for unique_annotaion in unique_annotaions:\n",
    "    print(unique_annotaion, ': ', annotations.count(unique_annotaion))\n",
    "\n",
    "print(len(annotations))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "datapath_training_parsed = CFG.datapath_training_parsed\n",
    "datapath_val_parsed = CFG.datapath_val_parsed\n",
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(datapath_training_parsed)\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(datapath_val_parsed)\n",
    "\n",
    "# train_dataset = CustomDataset(datapath_dd_training, caption_path_training, tokenizer, max_seq_length)\n",
    "train_dataset = CustomDataset(datapath_training_parsed, caption_path_training, tokenizer, CFG.max_seq_length)\n",
    "val_dataset  = CustomDataset(datapath_val_parsed, caption_path_val, tokenizer, CFG.max_seq_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(val_dataloader):\n",
    "    if os.name == 'nt': \n",
    "        os.system('cls')\n",
    "    else:\n",
    "        os.system('clear')\n",
    "\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens.shape)\n",
    "    print(\"gpt_mask: \", batch.gpt_mask.shape)\n",
    "    print(batch.instruction[0])\n",
    "    print(\"actions: \", batch.actions.shape)\n",
    "    print(\"observations: \", batch.observations.shape)\n",
    "    print(\"batch at index done: \", idx)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=CFG.device, jit=True)\n",
    "clip_text_encoder = clip_model.encode_text\n",
    "clip_text_features = clip_text_encoder(clip.tokenize(\"grasp the blue block, then rotate it left\").to(CFG.device))   #.detach().cpu().numpy()[()].squeeze(0)\n",
    "\n",
    "print(clip_text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: ClipCaptionModel):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_dataloader):\n",
    "\n",
    "            data.observations = data.observations.to(CFG.device)\n",
    "            data.actions = data.actions.to(CFG.device)\n",
    "            # data.instruction = data.instruction.to(CFG.device)\n",
    "            # data.instruction = (clip_text_encoder(clip.tokenize(data.instruction).to(CFG.device)).detach().cpu().numpy()).to(CFG.device)\n",
    "            data.instruction = clip_text_encoder(clip.tokenize(data.instruction).to(CFG.device)).to(CFG.device)\n",
    "            data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "            data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            logits = outputs.logits[:, data.observations.shape[1] - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), data.gpt_tokens.flatten(), ignore_index=0)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_dataloader)\n",
    "\n",
    "def train(model: ClipCaptionModel,\n",
    "          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    epochs = CFG.epochs\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            data.observations = data.observations.to(CFG.device)\n",
    "            data.actions = data.actions.to(CFG.device)\n",
    "            data.instruction = clip_text_encoder(clip.tokenize(data.instruction).to(CFG.device)).to(CFG.device)\n",
    "            data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "            data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "\n",
    "            logits = outputs.logits[:, data.observations.shape[1] - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), data.gpt_tokens.flatten(), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            progress.update()\n",
    "            if (idx+1) % 20 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "                # val_loss = validate(model)\n",
    "                # wandb.log({\"val_loss\": val_loss})\n",
    "        progress.close()\n",
    "        if epoch % 1 == 0 or epoch == epochs - 1:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"clipcalvin\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "prefix_length = 10\n",
    "prefix_length_clip = 10\n",
    "num_layers = 8\n",
    "prefix_dim = 512\n",
    "mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}[\"transformer\"]\n",
    "\n",
    "model = ClipCaptionModel(prefix_length, clip_length=prefix_length_clip, prefix_size=prefix_dim,\n",
    "                          num_layers=num_layers, mapping_type=mapping_type)\n",
    "\n",
    "\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=CFG.device, jit=True)\n",
    "clip_text_encoder = clip_model.encode_text\n",
    "\n",
    "train(model, output_dir=\"./checkpoints/hulccap/run2_with_stoptoken\", output_prefix=\"hulccap_prefix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hulccap_prefix-028.pt  loss:  0.33003602223470807\n",
      "hulccap_prefix-041.pt  loss:  0.4893910586833954\n",
      "hulccap_prefix-037.pt  loss:  0.4925169115886092\n",
      "hulccap_prefix-010.pt  loss:  0.5296978433616459\n",
      "hulccap_prefix-008.pt  loss:  0.6336398250423372\n",
      "hulccap_prefix-027.pt  loss:  0.34267887147143483\n",
      "hulccap_prefix-042.pt  loss:  0.6148938592523336\n",
      "hulccap_prefix-019.pt  loss:  0.4654630427248776\n",
      "hulccap_prefix-049.pt  loss:  0.47093211906030774\n",
      "hulccap_prefix-048.pt  loss:  0.4814322590827942\n",
      "hulccap_prefix-036.pt  loss:  0.3677130211144686\n",
      "hulccap_prefix-015.pt  loss:  0.4233202515169978\n",
      "hulccap_prefix-001.pt  loss:  1.4555744212120771\n",
      "hulccap_prefix-043.pt  loss:  0.4595368450973183\n",
      "hulccap_prefix-011.pt  loss:  0.5237803086638451\n",
      "hulccap_prefix-038.pt  loss:  0.3905929811298847\n",
      "hulccap_prefix-018.pt  loss:  0.4285810459405184\n",
      "hulccap_prefix-045.pt  loss:  0.5526004647836089\n",
      "hulccap_prefix-005.pt  loss:  0.7381061892956495\n",
      "hulccap_prefix-016.pt  loss:  0.45078537706285715\n",
      "hulccap_prefix-031.pt  loss:  0.4122629682533443\n",
      "hulccap_prefix-021.pt  loss:  0.3555849543772638\n",
      "hulccap_prefix-000.pt  loss:  3.242412965744734\n",
      "hulccap_prefix-003.pt  loss:  0.9945690603926778\n",
      "hulccap_prefix-007.pt  loss:  0.648568048607558\n",
      "hulccap_prefix-047.pt  loss:  0.5404016729444265\n",
      "hulccap_prefix-024.pt  loss:  0.35550000006332994\n",
      "hulccap_prefix-020.pt  loss:  0.38487500231713057\n",
      "hulccap_prefix-033.pt  loss:  0.3943716837093234\n",
      "hulccap_prefix_latest.pt  loss:  0.5361130158416927\n",
      "hulccap_prefix-023.pt  loss:  0.38657085970044136\n",
      "hulccap_prefix-009.pt  loss:  0.5955423852428794\n",
      "hulccap_prefix-017.pt  loss:  0.45077198231592774\n",
      "hulccap_prefix-026.pt  loss:  0.36990772653371096\n",
      "hulccap_prefix-044.pt  loss:  0.42259168927557766\n",
      "hulccap_prefix-025.pt  loss:  0.3824520790949464\n",
      "hulccap_prefix-012.pt  loss:  0.8742565582506359\n",
      "hulccap_prefix-032.pt  loss:  0.3828285257332027\n",
      "hulccap_prefix-046.pt  loss:  0.4344188733957708\n",
      "hulccap_prefix-002.pt  loss:  1.3242630437016487\n",
      "hulccap_prefix-022.pt  loss:  0.34525333857163787\n",
      "hulccap_prefix-004.pt  loss:  0.8166308039799333\n",
      "hulccap_prefix-013.pt  loss:  0.470561814494431\n",
      "hulccap_prefix-040.pt  loss:  0.35779553558677435\n",
      "hulccap_prefix-006.pt  loss:  0.6931518511846662\n",
      "hulccap_prefix-014.pt  loss:  0.4501283257268369\n",
      "hulccap_prefix-030.pt  loss:  0.33528616302646697\n",
      "hulccap_prefix-034.pt  loss:  0.3471942718606442\n",
      "hulccap_prefix-035.pt  loss:  0.3640506286174059\n",
      "hulccap_prefix-029.pt  loss:  0.3569827666506171\n",
      "hulccap_prefix-039.pt  loss:  0.45489967009052634\n",
      "best model:  hulccap_prefix-028.pt  loss:  0.33003602223470807\n"
     ]
    }
   ],
   "source": [
    "clip_model, _ = clip.load(\"ViT-B/32\", device=CFG.device, jit=True)\n",
    "clip_text_encoder = clip_model.encode_text\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "\n",
    "def evaluate_loss(path, filename, val_dataloader):\n",
    "\n",
    "    mapper_model = ClipCaptionModel(prefix_length=10, clip_length=10).to(CFG.device)\n",
    "    mapper_model.load_state_dict(torch.load(path, map_location=CFG.device))\n",
    "    mapper_model = mapper_model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in val_dataloader:\n",
    "\n",
    "        data.observations = data.observations.to(CFG.device)\n",
    "        data.actions = data.actions.to(CFG.device)\n",
    "        data.instruction = clip_text_encoder(clip.tokenize(data.instruction).to(CFG.device)).to(CFG.device)\n",
    "        data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "        data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "\n",
    "        outputs = mapper_model(data)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits = outputs.logits[:, data.observations.shape[1] - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), data.gpt_tokens.flatten(), ignore_index=0)    \n",
    "            total_loss += loss.item()\n",
    "    print(filename, ' loss: ', total_loss / len(val_dataloader))\n",
    "    return total_loss  / len(val_dataloader)\n",
    "\n",
    "        \n",
    "\n",
    "#model_dir = \"./checkpoints/hulccap/run_1/\"\n",
    "model_dir = \"./checkpoints/hulccap/run2_with_stoptoken/\"\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for filename in os.listdir(model_dir):\n",
    "    model_path = os.path.join(model_dir, filename)\n",
    "    current_loss = evaluate_loss(model_path, filename, val_dataloader)\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        best_model = filename\n",
    "\n",
    "print(\"best model: \", best_model, ' loss: ', best_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION: turn on the green light \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: pushplacepushtogglepushtoggle thepush off the led lamp \n",
      " \n",
      "INSTRUCTION: lift the pink block lying in the drawer \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: goliftgoliftgoliftgosllift the pink block in the drawer \n",
      "sl\n",
      "INSTRUCTION: slide the door to the left \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grslide the door to the left side\n",
      " The\n",
      "INSTRUCTION: pick up the red block from the table \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: gr pick lift pick thelift up the red block  the table \n",
      " \n",
      "INSTRUCTION: take the pink block and turn it right \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grrotgr\n",
      "gr the pink block and rotate it right \n",
      "\n",
      "INSTRUCTION: toggle the light switch to turn off the yellow light \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grmove the light switch to turn off the yellow light \n",
      "\n",
      "INSTRUCTION: move the light switch to turn on the yellow light \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: slmoveturnmove thepush the light switch to turn on the light \n",
      " \n",
      "INSTRUCTION: grasp the blue block, then lift it up \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grasp the blue block on then lift it up \n",
      " \n",
      "INSTRUCTION: in the slider pick up the blue block \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grinliftgr thelift the slider pick up the pink block \n",
      " \n",
      "INSTRUCTION: close the cabinet drawer \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grpush,push the cabinet drawer \n",
      "\n",
      "INSTRUCTION: grasp the door handle, then move the door to the left \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grasp the door handle, then move the door to the left \n",
      "gr \n",
      "INSTRUCTION: push the handle of the drawer \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grpush,push the handle of the drawer \n",
      "\n",
      "INSTRUCTION: lift the blue block on the shelf \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: gr the blue block lying the shelf \n",
      " \n",
      "INSTRUCTION: turn on the led \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: grslgrpushgrpush the led lamp\n",
      " left\n",
      "INSTRUCTION: in the slider pick up the red block \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: inlift the slider pick up the red block \n",
      " \n",
      "INSTRUCTION: push down the button to turn off the green light \n",
      "\n",
      "torch.Size([80, 50257])\n",
      "torch.Size([80])\n",
      "Generated Caption: pushmovepush the button to turn off the green light \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "terminate called without an active exception\n",
      "terminate called without an active exception\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=CFG.device, jit=True)\n",
    "clip_text_encoder = clip_model.encode_text\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# path_best = \"./checkpoints/hulccap/run_1/hulccap_prefix-032.pt\"\n",
    "path_best = \"./checkpoints/hulccap/run2_with_stoptoken/hulccap_prefix-028.pt\"\n",
    "best_model = ClipCaptionModel(prefix_length=10, clip_length=10).to(CFG.device)\n",
    "best_model.load_state_dict(torch.load(path_best, map_location=CFG.device))\n",
    "best_model = best_model.eval()\n",
    "\n",
    "for data in val_dataloader:\n",
    "\n",
    "    instruction_ground = data.instruction\n",
    "\n",
    "    data.observations = data.observations.to(CFG.device)\n",
    "    data.actions = data.actions.to(CFG.device)\n",
    "    data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "    data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "\n",
    "    outputs = best_model(data)\n",
    "    \n",
    "    for i in range(len(data.instruction)):\n",
    "        print(\"INSTRUCTION:\", instruction_ground[i])\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_token = outputs.logits[i].argmax(dim=-1)\n",
    "            print(outputs.logits[i].shape)\n",
    "            print(generated_token.shape)\n",
    "\n",
    "            # remove duplicates\n",
    "            result = []\n",
    "            for i in range(len(generated_token)):\n",
    "                word = generated_token[i]\n",
    "                if word not in result or result[-1] != word:\n",
    "                    result.append(word)\n",
    "            result_tensor = torch.tensor(result)\n",
    "\n",
    "            generated_text = tokenizer.decode(result_tensor)\n",
    "            print(\"Generated Caption:\", generated_text)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION: turn on the green light \n",
      "\n",
      "['turn off the led light \\n', 'push the button to turn off the led light ', 'toggle the button to turn off the led light ', 'push the button to turn off the led \\n', 'turn off the led lamp \\n']\n",
      "INSTRUCTION: lift the pink block lying in the drawer \n",
      "\n",
      "['lift the pink block lying in the drawer \\n', 'lift the pink block in the drawer \\n', 'go towards the pink block in the drawer and lift', 'pick up the pink block lying in the drawer ', 'go towards the pink block in the drawer and pick']\n",
      "INSTRUCTION: slide the door to the left \n",
      "\n",
      "['slide the door to the left side \\n', 'grasp the door handle, then slide the door', 'move the door all the way to the left and', 'push the door to the left side \\n', 'grasp the door handle and slide the door to']\n",
      "INSTRUCTION: pick up the red block from the table \n",
      "\n",
      "['grasp the red block on the table, then', 'grasp the red block on the table and lift', 'lift the red block from the table \\n', 'lift the red block \\n', 'pick up the red block \\n']\n",
      "INSTRUCTION: take the pink block and turn it right \n",
      "\n",
      "['take the pink block and rotate it right \\n', 'grasp the pink block, then rotate it right', 'grasp the pink block and rotate it right ', 'take the pink block and turn it right \\n', 'rotate right the pink block \\n']\n",
      "INSTRUCTION: toggle the light switch to turn off the yellow light \n",
      "\n",
      "['move the light switch to turn off the yellow light', 'turn off the yellow lamp \\n', 'move the light switch to turn off the light bulb', 'slide down the switch \\n', 'move down the switch \\n']\n",
      "INSTRUCTION: move the light switch to turn on the yellow light \n",
      "\n",
      "['move the light switch to turn on the light bulb', 'move the light switch to turn on the yellow light', 'push the switch upwards \\n', 'toggle the light switch to turn on the yellow light', 'toggle the light switch to turn on the light bulb']\n",
      "INSTRUCTION: grasp the blue block, then lift it up \n",
      "\n",
      "['grasp the blue block on the table, then', 'lift the blue block from the table \\n', 'pick up the blue block on the table \\n', 'grasp the blue block on the table and lift', 'pick up the blue block from the table \\n']\n",
      "INSTRUCTION: in the slider pick up the blue block \n",
      "\n",
      "['lift the blue block lying in the slider \\n', 'lift the pink block lying in the slider \\n', 'lift the blue block lying in the sliding cabinet ', 'grasp the blue block lying in the slider ', 'grasp the pink block lying in the slider ']\n",
      "INSTRUCTION: close the cabinet drawer \n",
      "\n",
      "['push the handle of the drawer \\n', 'grasp the handle of the drawer, then close', 'push the drawer \\n', 'go close the drawer \\n', 'grasp the handle of the drawer and close it']\n",
      "INSTRUCTION: grasp the door handle, then move the door to the left \n",
      "\n",
      "['slide the door to the left side \\n', 'grasp the door handle, then move the door', 'grasp the door handle, then slide the door', 'move the door to the left side \\n', 'move the door all the way to the left and']\n",
      "INSTRUCTION: push the handle of the drawer \n",
      "\n",
      "['push the handle of the drawer \\n', 'grasp the handle of the drawer, then close', 'grasp the handle of the drawer and close it', 'push the drawer \\n', 'go close the drawer \\n']\n",
      "INSTRUCTION: lift the blue block on the shelf \n",
      "\n",
      "['lift the blue block lying in the slider \\n', 'grasp the blue block lying in the slider ', 'grasp the blue block lying in the cabinet ', 'in the slider grasp the blue block \\n', 'in the cabinet grasp the blue block \\n']\n",
      "INSTRUCTION: turn on the led \n",
      "\n",
      "['toggle the button to turn on the led light ', 'grasp the pink block on the table and lift', 'grasp the pink block on the table, then', 'push the pink block to the left \\n', 'push left the pink block \\n']\n",
      "INSTRUCTION: in the slider pick up the red block \n",
      "\n",
      "['lift the red block lying in the slider \\n', 'in the slider pick up the red block \\n', 'pick up the red block in the sliding cabinet ', 'lift the red block lying in the sliding cabinet ', 'grasp the red block lying in the slider ']\n",
      "INSTRUCTION: push down the button to turn off the green light \n",
      "\n",
      "['push the button to turn off the led light ', 'push the door to the right side \\n', 'push the door to the left side \\n', 'move the door to the left side \\n', 'slide the door to the right side \\n']\n",
      "INSTRUCTION: go towards the red block in the drawer and grasp it \n",
      "\n",
      "['pick up the red block from the drawer \\n', 'go towards the red block in the drawer and lift', 'pick up the red block in the drawer \\n', 'grasp the red block in the drawer \\n', 'grasp the red block from the drawer \\n']\n",
      "INSTRUCTION: go slide the red block to the left \n",
      "\n",
      "['slide the red block to the left \\n', 'slide left the red block \\n', 'sweep the red block to the left \\n', 'go push the red block to the left \\n', 'push the red block to the left \\n']\n",
      "INSTRUCTION: put the object in the sliding cabinet \n",
      "\n",
      "['place the grasped object in the sliding cabinet \\n', 'put the block in the sliding cabinet \\n', 'place the block in the sliding cabinet \\n', 'put the grasped object in the sliding cabinet \\n', 'place the object in the sliding cabinet \\n']\n",
      "INSTRUCTION: turn on the led light \n",
      "\n",
      "['push the sliding door to the right \\n', 'push the button to turn off the led \\n', 'push the button to turn off the led light ', 'push the door to the right side \\n', 'push the button to turn on the led \\n']\n",
      "INSTRUCTION: rotate the red block towards the left \n",
      "\n",
      "['take the red block and rotate it left \\n', 'rotate the red block to the left \\n', 'rotate the red block towards the left \\n', 'grasp the red block, then rotate it left', 'grasp the red block and rotate it left ']\n",
      "INSTRUCTION: stack blocks on top of each other \n",
      "\n",
      "['put the grasped block on top of a block ', 'put the block on top of another block \\n', 'stack the block on top of another block \\n', 'place the grasped block on top of another block ', 'stack the object on top of another object \\n']\n",
      "INSTRUCTION: grasp the red block lying in the drawer \n",
      "\n",
      "['pick up the red block in the drawer \\n', 'grasp the red block in the drawer \\n', 'pick up the red block from the drawer \\n', 'lift the red block in the drawer \\n', 'go towards the red block in the drawer and lift']\n",
      "INSTRUCTION: grasp the pink block lying in the cabinet \n",
      "\n",
      "['lift the pink block lying in the slider \\n', 'in the slider pick up the pink block \\n', 'in the slider grasp the pink block \\n', 'grasp the pink block lying in the slider ', 'lift the pink block lying in the sliding cabinet ']\n",
      "INSTRUCTION: store the object in the drawer \n",
      "\n",
      "['store the block in the drawer \\n', 'store the object in the drawer \\n', 'move to the drawer, then store the object ', 'place the block in the drawer \\n', 'place the object in the drawer \\n']\n",
      "INSTRUCTION: move the door all the way to the right \n",
      "\n",
      "['grasp the door handle, then slide the door', 'grasp the door handle and slide the door to', 'slide the door all the way to the right', 'slide the door to the right side \\n', 'move the door all the way to the right ']\n",
      "INSTRUCTION: toggle the button to turn on the green light \n",
      "\n",
      "['toggle the button to turn off the led light ', 'push the button to turn off the led light ', 'turn off the led light \\n', 'toggle the button to turn off the led \\n', 'toggle the button to turn on the led light ']\n",
      "INSTRUCTION: push the door to the left side \n",
      "\n",
      "['grasp the door handle, then slide the door', 'grasp the door handle and move the door to', 'grasp the door handle, then move the door', 'push the door to the left side \\n', 'move the door all the way to the left and']\n",
      "INSTRUCTION: place the object in the sliding cabinet \n",
      "\n",
      "['put the grasped block on top of a block ', 'slide right the red block \\n', 'push the red block to the right \\n', 'go to the tower of blocks and take off the', 'slide the red block to the right \\n']\n",
      "INSTRUCTION: push the button to turn on the green light \n",
      "\n",
      "['toggle the button to turn off the green light ', 'push the button to turn off the led light ', 'toggle the button to turn off the led light ', 'turn off the green light \\n', 'turn off the led light \\n']\n",
      "INSTRUCTION: push down the button to turn on the led light \n",
      "\n",
      "['push the button to turn off the led light ', 'toggle the button to turn off the led light ', 'push the button to turn on the led light ', 'push the button to turn off the led \\n', 'toggle the button to turn off the led \\n']\n",
      "INSTRUCTION: stack the blocks \n",
      "\n",
      "['put the grasped block on top of a block ', 'stack the object on top of another object \\n', 'take off the block that is on top of the', 'take off the stacked block \\n', 'stack the blocks \\n']\n",
      "INSTRUCTION: grasp the blue block on the table, then lift it up \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m behaviour_encoding \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39mbehaviour_encoder(src)\n\u001b[1;32m     95\u001b[0m prefix_embed \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39mproject_to_gpt(behaviour_encoding)\n\u001b[0;32m---> 97\u001b[0m generated_caption \u001b[39m=\u001b[39m  beamsearch(best_model, tokenizer, prefix_embed)\n\u001b[1;32m     98\u001b[0m \u001b[39mprint\u001b[39m(generated_caption)\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mbeamsearch\u001b[0;34m(model, tokenizer, embed, beam_size, stop_token)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m         outputs \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39;49mgpt(inputs_embeds\u001b[39m=\u001b[39;49mgenerated)\n\u001b[1;32m     16\u001b[0m         logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     17\u001b[0m         logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msoftmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mlog()\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1077\u001b[0m     input_ids,\n\u001b[1;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:202\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull([], mask_value, dtype\u001b[39m=\u001b[39mattn_weights\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mto(attn_weights\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 202\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(causal_mask, attn_weights\u001b[39m.\u001b[39;49mto(attn_weights\u001b[39m.\u001b[39;49mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[39m# Apply the attention mask\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m attention_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=CFG.num_workers)\n",
    "\n",
    "def beamsearch(model, tokenizer, embed, beam_size: int = 5, stop_token: str = '\\n'):\n",
    "    model.eval()\n",
    "    scores = None\n",
    "    tokens = None\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    seq_lengths = torch.ones(beam_size, device=CFG.device)\n",
    "    is_stopped = torch.zeros(beam_size, device=CFG.device, dtype=torch.bool)\n",
    "    generated = embed\n",
    "    with torch.no_grad():\n",
    "        for i in range(20):\n",
    "            outputs = best_model.gpt(inputs_embeds=generated)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            logits = logits.softmax(-1).log()\n",
    "            #print(logits.shape)\n",
    "            if scores is None:\n",
    "                scores, next_tokens = logits.topk(beam_size, -1)\n",
    "                #print(scores)\n",
    "                #print(next_tokens)\n",
    "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
    "                #print(generated.shape)\n",
    "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
    "                if tokens is None:\n",
    "                    tokens = next_tokens\n",
    "                else:\n",
    "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
    "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "            else:\n",
    "                logits[is_stopped] = -float(np.inf)\n",
    "                logits[is_stopped, 0] = 0\n",
    "                scores_sum = scores[:, None] + logits\n",
    "                seq_lengths[~is_stopped] += 1\n",
    "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
    "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
    "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
    "                seq_lengths = seq_lengths[next_tokens_source]\n",
    "                next_tokens = next_tokens % scores_sum.shape[1]\n",
    "                next_tokens = next_tokens.unsqueeze(1)\n",
    "                tokens = tokens[next_tokens_source]\n",
    "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "                generated = generated[next_tokens_source]\n",
    "                scores = scores_sum_average * seq_lengths\n",
    "                is_stopped = is_stopped[next_tokens_source]\n",
    "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
    "            if is_stopped.all():\n",
    "                break\n",
    "    scores = scores / seq_lengths\n",
    "    output_list = tokens.cpu().numpy()\n",
    "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
    "    order = scores.argsort(descending=True)\n",
    "    output_texts = [output_texts[i] for i in order]\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=CFG.device, jit=True)\n",
    "clip_text_encoder = clip_model.encode_text\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "#path_best = \"./checkpoints/hulccap/run_1/hulccap_prefix-032.pt\"\n",
    "path_best = \"./checkpoints/hulccap/run2_with_stoptoken/hulccap_prefix-028.pt\"\n",
    "\n",
    "best_model = ClipCaptionModel(prefix_length=10, clip_length=10).to(CFG.device)\n",
    "best_model.load_state_dict(torch.load(path_best, map_location=CFG.device))\n",
    "# best_model = best_model.eval()\n",
    "\n",
    "for data in val_dataloader:\n",
    "\n",
    "    instruction_ground = data.instruction\n",
    "\n",
    "    data.observations = data.observations.to(CFG.device)\n",
    "    data.actions = data.actions.to(CFG.device)\n",
    "    data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "    data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "\n",
    "    # outputs = best_model(data)\n",
    "    \n",
    "    for i in range(len(data.instruction)):\n",
    "        print(\"INSTRUCTION:\", instruction_ground[i])\n",
    "        \n",
    "      #  generated_token = outputs.logits[i].argmax(dim=-1)\n",
    "        \"\"\"\n",
    "        tokens = torch.tensor(tokenizer.encode(\"take\"))\n",
    "        tokens = tokens.unsqueeze(0).to(CFG.device)\n",
    "        generated = best_model.gpt.transformer.wte(tokens)\n",
    "        inputs = best_model.gpt.transformer.wte(tokens)\n",
    "        \"\"\"\n",
    "        src = AttrDict(observations=data.observations, actions=data.actions)\n",
    "        behaviour_encoding = best_model.behaviour_encoder(src)\n",
    "        prefix_embed = best_model.project_to_gpt(behaviour_encoding)\n",
    "\n",
    "        generated_caption =  beamsearch(best_model, tokenizer, prefix_embed)\n",
    "        print(generated_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
