{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-26 00:30:09.743679: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:09.889526: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:09.891285: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 00:30:10.923419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from r3m import load_r3m\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
    "\n",
    "from dataset import CustomDataset, AttrDict\n",
    "\n",
    "# datapath_dd_training = '/media/tim/6f37312f-8eb4-400c-a4e7-e229c18bbf2c/datasets/calvin_debug_dataset/training'\n",
    "# datapath_dd_training = '/media/tim/E/datasets/task_D_D/training'\n",
    "# datapath_dd_val = '/media/tim/E/datasets/task_D_D/validation'\n",
    "datapath_dd_training = '/home/tim/calvin_debug_dataset/training'\n",
    "datapath_dd_val = '/home/tim/calvin_debug_dataset/validation'\n",
    "# datapath_dd_training = '/media/tim/6f37312f-8eb4-400c-a4e7-e229c18bbf2c/datasets/hulc2/unprocessed/real_world/500k_all_tasks_dataset_15hz'\n",
    "mp.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Calvin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "\n",
    "    #parser = ArgumentParser(description=\"Interactive visualization of CALVIN dataset\")\n",
    "    #parser.add_argument(\"path\", type=str, help=\"Path to dir containing scene_info.npy\")\n",
    "    #parser.add_argument(\"-d\", \"--data\", nargs=\"*\", default=[\"rgb_static\", \"rgb_gripper\"], help=\"Data to visualize\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    path = datapath_dd_training\n",
    "    data = [\"rgb_static\", \"rgb_gripper\"]\n",
    "\n",
    "    if not Path(path).is_dir():\n",
    "        print(f\"Path {path} is either not a directory, or does not exist.\")\n",
    "        exit()\n",
    "\n",
    "    indices = next(iter(np.load(f\"{path}/scene_info.npy\", allow_pickle=True).item().values()))\n",
    "    indices = list(range(indices[0], indices[1] + 1))\n",
    "\n",
    "    scene_info = np.load(f\"{path}/scene_info.npy\", allow_pickle=True)\n",
    "    print(scene_info)\n",
    "\n",
    "    annotations = np.load(f\"{path}/lang_annotations/auto_lang_ann.npy\", allow_pickle=True).item()\n",
    "    annotations = list(zip(annotations[\"info\"][\"indx\"], annotations[\"language\"][\"ann\"]))\n",
    "    print(annotations)\n",
    "    print(len(annotations))\n",
    "\n",
    "    idx = 0\n",
    "    # idx = 60000\n",
    "    ann_idx = -1\n",
    "\n",
    "    while True:\n",
    "        t = np.load(f\"{path}/episode_{indices[idx]:07d}.npz\", allow_pickle=True)\n",
    "\n",
    "        for d in data:\n",
    "            if d not in t:\n",
    "                print(f\"Data {d} cannot be found in transition\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.resize(t[d], (400, 400))\n",
    "            cv2.imshow(d, img[:, :, ::-1])\n",
    "\n",
    "        for n, ((low, high), ann) in enumerate(annotations):\n",
    "            if indices[idx] >= low and indices[idx] <= high:\n",
    "                if n != ann_idx:\n",
    "                    print(f\"{ann}\")\n",
    "                    ann_idx = n\n",
    "\n",
    "        # user_input = input(\"Enter something: \")\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(0)\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        elif key == 83:  # Right arrow\n",
    "            idx = (idx + 1) % len(indices)\n",
    "        elif key == 81:  # Left arrow\n",
    "            idx = (len(indices) + idx - 1) % len(indices)\n",
    "        else:\n",
    "            print(f'Unrecognized keycode \"{key}\"')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calvin_scene_D': [358482, 361252]}\n",
      "[((358656, 358720), 'move the light switch to turn on the yellow light'), ((359714, 359757), 'sweep the pink block to the right'), ((360978, 361011), 'place the block in the sliding cabinet'), ((358728, 358792), 'pick up the red block from the table'), ((359534, 359598), 'in the slider grasp the blue block'), ((360571, 360635), 'slide down the switch'), ((358729, 358793), 'pick up the red block on the table'), ((359069, 359103), 'place in slider'), ((360575, 360639), 'turn off the light bulb')]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/tim/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/tim/E/datasets/task_D_D/training/episode_0053819.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m datapath_test\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/media/tim/E/datasets/task_D_D/training/episode_0053819.npz\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(datapath_test)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39m(data\u001b[39m.\u001b[39mkeys()))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mactions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/tim/E/datasets/task_D_D/training/episode_0053819.npz'"
     ]
    }
   ],
   "source": [
    "datapath_test= '/media/tim/E/datasets/task_D_D/training/episode_0053819.npz'\n",
    "data = np.load(datapath_test)\n",
    "print(list(data.keys()))\n",
    "print(data['actions'].shape)\n",
    "print(data['rgb_static'].shape)\n",
    "print(data['rgb_gripper'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning\n",
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/.local/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/tim/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tim/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "max_seq_length = 16\n",
    "\n",
    "path = '/home/tim/calvin_debug_dataset/'\n",
    "\n",
    "datapath_dd_training = '{}training'.format(path)\n",
    "datapath_dd_val = '{}validation'.format(path)\n",
    "caption_path_training = '{}training/lang_annotations/auto_lang_ann.npy'.format(path)\n",
    "caption_path_val = '{}validation/lang_annotations/auto_lang_ann.npy'.format(path)\n",
    "\n",
    "train_dataset = CustomDataset(datapath_dd_training, caption_path_training, tokenizer, max_seq_length)\n",
    "val_dataset  = CustomDataset(datapath_dd_val, caption_path_val, tokenizer, max_seq_length)\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 00:30:23.990830: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:24.033202: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:24.033566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 00:30:25.004452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-26 00:30:28.084354: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:28.126621: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:28.127065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 00:30:28.960723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-26 00:30:32.027097: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:32.071800: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:32.072270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 00:30:32.877115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-26 00:30:36.218353: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:36.264234: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 00:30:36.264641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 00:30:37.115356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_tokens:  torch.Size([4, 16])\n",
      "gpt_mask:  torch.Size([4, 16])\n",
      "gpt_tokens:  tensor([15344,   572,   262,  1657, 28287,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "gpt_mask:  tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "turn off the light bulb\n",
      "actions:  torch.Size([4, 64, 7])\n",
      "observations:  torch.Size([4, 6, 2048])\n",
      "observations:  tensor([[[1.0301e-03, 0.0000e+00, 1.1530e-02,  ..., 7.2582e-03,\n",
      "          5.0681e-02, 8.3216e-02],\n",
      "         [6.6977e-03, 0.0000e+00, 5.4827e-03,  ..., 1.1312e-02,\n",
      "          1.5491e-02, 8.1042e-02],\n",
      "         [4.1995e-03, 0.0000e+00, 3.8414e-03,  ..., 1.0868e-02,\n",
      "          3.4670e-02, 9.4621e-02],\n",
      "         [1.0301e-03, 0.0000e+00, 1.1530e-02,  ..., 7.2582e-03,\n",
      "          5.0681e-02, 8.3216e-02],\n",
      "         [6.6977e-03, 0.0000e+00, 5.4827e-03,  ..., 1.1312e-02,\n",
      "          1.5491e-02, 8.1042e-02],\n",
      "         [4.1995e-03, 0.0000e+00, 3.8414e-03,  ..., 1.0868e-02,\n",
      "          3.4670e-02, 9.4621e-02]],\n",
      "\n",
      "        [[6.1333e-03, 0.0000e+00, 1.8844e-05,  ..., 1.8776e-03,\n",
      "          1.7394e-02, 1.0603e-01],\n",
      "         [4.5270e-03, 0.0000e+00, 1.2898e-03,  ..., 1.0064e-02,\n",
      "          9.7841e-03, 1.0532e-01],\n",
      "         [8.3128e-03, 0.0000e+00, 0.0000e+00,  ..., 8.4324e-03,\n",
      "          1.8852e-02, 6.3909e-02],\n",
      "         [6.1333e-03, 0.0000e+00, 1.8844e-05,  ..., 1.8776e-03,\n",
      "          1.7394e-02, 1.0603e-01],\n",
      "         [4.5270e-03, 0.0000e+00, 1.2898e-03,  ..., 1.0064e-02,\n",
      "          9.7841e-03, 1.0532e-01],\n",
      "         [8.3128e-03, 0.0000e+00, 0.0000e+00,  ..., 8.4324e-03,\n",
      "          1.8852e-02, 6.3909e-02]],\n",
      "\n",
      "        [[5.7400e-03, 0.0000e+00, 1.0501e-03,  ..., 3.9490e-03,\n",
      "          5.5284e-02, 8.5073e-02],\n",
      "         [1.0513e-03, 0.0000e+00, 2.6380e-04,  ..., 5.9566e-03,\n",
      "          4.9581e-02, 9.3074e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4946e-03,\n",
      "          5.9586e-03, 9.1572e-02],\n",
      "         [5.7400e-03, 0.0000e+00, 1.0501e-03,  ..., 3.9490e-03,\n",
      "          5.5284e-02, 8.5073e-02],\n",
      "         [1.0513e-03, 0.0000e+00, 2.6380e-04,  ..., 5.9566e-03,\n",
      "          4.9581e-02, 9.3074e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4946e-03,\n",
      "          5.9586e-03, 9.1572e-02]],\n",
      "\n",
      "        [[6.0819e-03, 0.0000e+00, 6.9883e-05,  ..., 1.8993e-03,\n",
      "          1.7522e-02, 1.0544e-01],\n",
      "         [4.4171e-03, 0.0000e+00, 1.3115e-03,  ..., 1.0534e-02,\n",
      "          1.2568e-02, 1.0665e-01],\n",
      "         [8.5904e-03, 0.0000e+00, 0.0000e+00,  ..., 8.1452e-03,\n",
      "          1.8993e-02, 6.5914e-02],\n",
      "         [6.0819e-03, 0.0000e+00, 6.9883e-05,  ..., 1.8993e-03,\n",
      "          1.7522e-02, 1.0544e-01],\n",
      "         [4.4171e-03, 0.0000e+00, 1.3115e-03,  ..., 1.0534e-02,\n",
      "          1.2568e-02, 1.0665e-01],\n",
      "         [8.5904e-03, 0.0000e+00, 0.0000e+00,  ..., 8.1452e-03,\n",
      "          1.8993e-02, 6.5914e-02]]])\n",
      "#############################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens.shape)\n",
    "    print(\"gpt_mask: \", batch.gpt_mask.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens[0])\n",
    "    print(\"gpt_mask: \", batch.gpt_mask[0])\n",
    "   # if(torch.all(torch.eq(batch.gpt_tokens, batch.token))):\n",
    "   #     print(\"correct\")\n",
    "    print(batch.instruction[0])\n",
    "    print(\"actions: \", batch.actions.shape)\n",
    "    print(\"rgb_static: \", batch.rgb_static.shape)\n",
    "    print(\"rgb_gripper: \", batch.rgb_gripper.shape)\n",
    "    print(\"#############################################\")\n",
    "    break\n",
    "\"\"\"\n",
    "\n",
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens.shape)\n",
    "    print(\"gpt_mask: \", batch.gpt_mask.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens[0])\n",
    "    print(\"gpt_mask: \", batch.gpt_mask[0])\n",
    "   # if(torch.all(torch.eq(batch.gpt_tokens, batch.token))):\n",
    "   #     print(\"correct\")\n",
    "    print(batch.instruction[0])\n",
    "    print(\"actions: \", batch.actions.shape)\n",
    "    print(\"observations: \", batch.observations.shape)\n",
    "    print(\"observations: \", batch.observations)\n",
    "    print(\"#############################################\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Captioning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MappingType(Enum):\n",
    "    MLP = 'mlp'\n",
    "    Transformer = 'transformer'\n",
    "\n",
    "class BehaviourEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        # self.pos_encoder = PositionalEncoding(CFG.d_model, dropout=CFG.dropout)\n",
    "        self.pos_encoder = PositionalEncoding(2050, dropout=0.1)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=CFG.d_model, nhead=CFG.n_heads, batch_first=True, dim_feedforward=CFG.d_ff, dropout=CFG.dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=CFG.n_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        image_features = src.observations # images: [batch_size, sequence_length, 2048]\n",
    "        actions = src.actions\n",
    "\n",
    "        src_key_padding_mask = (image_features.mean(dim=2)==0.0)\n",
    "        features = torch.cat((image_features, actions), dim=-1)\n",
    "\n",
    "        # Transformer encoder\n",
    "        # add positional encoding\n",
    "        features = features * math.sqrt(CFG.d_model)mp.set_start_method('spawn')\n",
    "        features = self.pos_encoder(features)\n",
    "        behaviour_encoding = self.transformer_encoder(features, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return behaviour_encoding\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
    "                                                                     clip_length, num_layers)\n",
    "        self.behaviour_encoder = BehaviourEncoder()\n",
    "        self.project_to_gpt = nn.Linear(514, self.gpt_embedding_size).to(\"cuda\")\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, data: AttrDict):\n",
    "\n",
    "        tokens = data.gpt_tokens\n",
    "        gpt_mask = data.gpt_mask\n",
    "        labels = None\n",
    "\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        behaviour_encoding  = self.behaviour_encoder(data)\n",
    "        \n",
    "        # !!!!!!!!!!!\n",
    "        behaviour_encoder_padding_mask = ~(data.rgb_static.mean(dim=2)==0.0) * 1.0\n",
    "\n",
    "        prefix_projections = self.project_to_gpt(behaviour_encoding)\n",
    "\n",
    "        total_mask = torch.cat((behaviour_encoder_padding_mask, embedding_text), dim=1)\n",
    "\n",
    "        ######################################\n",
    "        # prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=total_mask)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
