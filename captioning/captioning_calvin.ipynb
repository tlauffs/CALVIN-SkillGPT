{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/cap-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-30 13:10:06.469884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from r3m import load_r3m\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
    "from torch.nn import functional as nnf\n",
    "import wandb\n",
    "\n",
    "from dataset import CustomDataset, AttrDict \n",
    "\n",
    "import config as CFG\n",
    "\n",
    "\n",
    "mp.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Calvin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "\n",
    "    #parser = ArgumentParser(description=\"Interactive visualization of CALVIN dataset\")\n",
    "    #parser.add_argument(\"path\", type=str, help=\"Path to dir containing scene_info.npy\")\n",
    "    #parser.add_argument(\"-d\", \"--data\", nargs=\"*\", default=[\"rgb_static\", \"rgb_gripper\"], help=\"Data to visualize\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    path = CFG.datapath_training\n",
    "    data = [\"rgb_static\", \"rgb_gripper\"]\n",
    "\n",
    "    if not Path(path).is_dir():\n",
    "        print(f\"Path {path} is either not a directory, or does not exist.\")\n",
    "        exit()\n",
    "\n",
    "    indices = next(iter(np.load(f\"{path}/scene_info.npy\", allow_pickle=True).item().values()))\n",
    "    indices = list(range(indices[0], indices[1] + 1))\n",
    "\n",
    "    scene_info = np.load(f\"{path}/scene_info.npy\", allow_pickle=True)\n",
    "    print(scene_info)\n",
    "\n",
    "    annotations = np.load(f\"{path}/lang_annotations/auto_lang_ann.npy\", allow_pickle=True).item()\n",
    "    annotations = list(zip(annotations[\"info\"][\"indx\"], annotations[\"language\"][\"ann\"]))\n",
    "    print(annotations)\n",
    "    print(len(annotations))\n",
    "\n",
    "    idx = 0\n",
    "    # idx = 60000\n",
    "    ann_idx = -1\n",
    "\n",
    "    while True:\n",
    "        t = np.load(f\"{path}/episode_{indices[idx]:07d}.npz\", allow_pickle=True)\n",
    "\n",
    "        for d in data:\n",
    "            if d not in t:\n",
    "                print(f\"Data {d} cannot be found in transition\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.resize(t[d], (400, 400))\n",
    "            cv2.imshow(d, img[:, :, ::-1])\n",
    "\n",
    "        for n, ((low, high), ann) in enumerate(annotations):\n",
    "            if indices[idx] >= low and indices[idx] <= high:\n",
    "                if n != ann_idx:\n",
    "                    print(f\"{ann}\")\n",
    "                    ann_idx = n\n",
    "\n",
    "        # user_input = input(\"Enter something: \")\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(0)\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        elif key == 83:  # Right arrow\n",
    "            idx = (idx + 1) % len(indices)\n",
    "        elif key == 81:  # Left arrow\n",
    "            idx = (len(indices) + idx - 1) % len(indices)\n",
    "        else:\n",
    "            print(f'Unrecognized keycode \"{key}\"')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath_test= '.././test_data/D_D/task_D_D_episode.npz'\n",
    "data = np.load(datapath_test)\n",
    "print(list(data.keys()))\n",
    "print(data['actions'].shape)\n",
    "print(data['rel_actions'].shape)\n",
    "print(data['rgb_static'].shape)\n",
    "print(data['rgb_gripper'].shape)\n",
    "print(data['scene_obs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Captioning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MappingType(Enum):\n",
    "    MLP = 'mlp'\n",
    "    Transformer = 'transformer'\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        # pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class BehaviourEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.pos_encoder = PositionalEncoding(CFG.d_model, dropout=CFG.dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=CFG.d_model, nhead=CFG.n_heads, batch_first=True, dim_feedforward=CFG.d_ff, dropout=CFG.dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=CFG.n_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        image_features = src.observations # images: [batch_size, sequence_length, 2048]\n",
    "        actions = src.actions\n",
    "\n",
    "        src_key_padding_mask = (image_features.mean(dim=2)==0.0)\n",
    "        features = torch.cat((image_features, actions), dim=-1)\n",
    "\n",
    "        # Transformer encoder\n",
    "        # add positional encoding\n",
    "        features = features * math.sqrt(CFG.d_model)\n",
    "        features = self.pos_encoder(features)\n",
    "        behaviour_encoding = self.transformer_encoder(features, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return behaviour_encoding\n",
    "\n",
    "class TransformerMapper(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
    "        super(TransformerMapper, self).__init__()\n",
    "        self.clip_length = clip_length\n",
    "        #self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=8, batch_first=True, dim_feedforward=int(dim_embedding * 2), dropout=0.0)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
    "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
    "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
    "        prefix = torch.cat((x, prefix), dim=1)\n",
    "        out = self.transformer(prefix)[:, self.clip_length:]\n",
    "        return out\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
    "                                                                     clip_length, num_layers)\n",
    "        self.behaviour_encoder = BehaviourEncoder()\n",
    "        self.project_to_gpt = nn.Linear(514, self.gpt_embedding_size).to(\"cuda\")\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, data: AttrDict):\n",
    "\n",
    "        tokens = data.gpt_tokens\n",
    "        gpt_mask = data.gpt_mask\n",
    "        labels = None\n",
    "\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        behaviour_encoding  = self.behaviour_encoder(data)\n",
    "        \n",
    "        behaviour_encoder_padding_mask = ~(data.observations.mean(dim=2)==0.0) * 1.0\n",
    "        prefix_projections = self.project_to_gpt(behaviour_encoding)\n",
    "\n",
    "        total_mask = torch.cat((behaviour_encoder_padding_mask, embedding_text), dim=1)\n",
    "\n",
    "        # prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=total_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find sutiable caption for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = CFG.datapath_training_parsed + \"/lang_annotations/auto_lang_ann.npy\"\n",
    "\n",
    "annotations = np.load(path, allow_pickle=True).item()\n",
    "annotations = annotations[\"language\"][\"ann\"]\n",
    "\n",
    "unique_annotaions = set(annotations)\n",
    "for unique_annotaion in unique_annotaions:\n",
    "    print(unique_annotaion, ': ', annotations.count(unique_annotaion))\n",
    "\n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "datapath_training_parsed = CFG.datapath_training_parsed\n",
    "datapath_val = CFG.datapath_val_parsed\n",
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(datapath_training_parsed)\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(datapath_val)\n",
    "\n",
    "# train_dataset = CustomDataset(datapath_dd_training, caption_path_training, tokenizer, max_seq_length)\n",
    "train_dataset = CustomDataset(datapath_training_parsed, caption_path_training, tokenizer, CFG.max_seq_length)\n",
    "val_dataset  = CustomDataset(datapath_val, caption_path_val, tokenizer, CFG.max_seq_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 13:11:36.654889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 13:11:39.222922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 13:11:41.751223: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 13:11:44.281970: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 13:11:46.808809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m      2\u001b[0m    \u001b[39m# print(\"token: \", batch.token.shape)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgpt_tokens: \u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m.\u001b[39mgpt_tokens\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgpt_mask: \u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m.\u001b[39mgpt_mask\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/cap-env/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39mwrite(fp\u001b[39m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 13:11:49.419013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens.shape)\n",
    "    print(\"gpt_mask: \", batch.gpt_mask.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens[0])\n",
    "    print(\"gpt_mask: \", batch.gpt_mask[0])\n",
    "   # if(torch.all(torch.eq(batch.gpt_tokens, batch.token))):\n",
    "   #     print(\"correct\")\n",
    "    print(batch.instruction[0])\n",
    "    print(\"actions: \", batch.actions.shape)\n",
    "    print(\"observations: \", batch.observations.shape)\n",
    "    # print(\"observations: \", batch.observations)\n",
    "    print(\"#############################################\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"clipcalvin\")\n",
    "\n",
    "def validate(model: ClipCaptionModel):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_dataloader):\n",
    "            data.observations = data.observations.to(CFG.device)\n",
    "            data.actions = data.actions.to(CFG.device)\n",
    "            # data.instruction = data.instruction.to(CFG.device)\n",
    "            data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "            data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            logits = outputs.logits[:, data.observations.shape[1] - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), data.gpt_tokens.flatten(), ignore_index=0)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_dataloader)\n",
    "\n",
    "def train(model: ClipCaptionModel,\n",
    "          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    epochs = CFG.epochs\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            data.observations = data.observations.to(CFG.device)\n",
    "            data.actions = data.actions.to(CFG.device)\n",
    "            # data.instruction = data.instruction.to(CFG.device)\n",
    "            data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "            data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            logits = outputs.logits[:, data.observations.shape[1] - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), data.gpt_tokens.flatten(), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            progress.update()\n",
    "            if (idx+1) % 2000 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "                val_loss = validate(model)\n",
    "                wandb.log({\"val_loss\": val_loss})\n",
    "        progress.close()\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
