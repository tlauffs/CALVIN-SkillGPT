{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-28 14:23:46.098211: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:23:46.262221: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:23:46.264051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 14:23:48.356178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from r3m import load_r3m\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
    "\n",
    "from dataset import CustomDataset, AttrDict\n",
    "\n",
    "# datapath_dd_training = '/media/tim/6f37312f-8eb4-400c-a4e7-e229c18bbf2c/datasets/calvin_debug_dataset/training'\n",
    "# datapath_dd_training = '/media/tim/E/datasets/task_D_D/training'\n",
    "# datapath_dd_val = '/media/tim/E/datasets/task_D_D/validation'\n",
    "datapath_dd_training = '/home/tim/calvin_debug_dataset/training'\n",
    "datapath_dd_val = '/home/tim/calvin_debug_dataset/validation'\n",
    "# datapath_dd_training = '/media/tim/6f37312f-8eb4-400c-a4e7-e229c18bbf2c/datasets/hulc2/unprocessed/real_world/500k_all_tasks_dataset_15hz'\n",
    "mp.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Calvin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "\n",
    "    #parser = ArgumentParser(description=\"Interactive visualization of CALVIN dataset\")\n",
    "    #parser.add_argument(\"path\", type=str, help=\"Path to dir containing scene_info.npy\")\n",
    "    #parser.add_argument(\"-d\", \"--data\", nargs=\"*\", default=[\"rgb_static\", \"rgb_gripper\"], help=\"Data to visualize\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    path = datapath_dd_training\n",
    "    data = [\"rgb_static\", \"rgb_gripper\"]\n",
    "\n",
    "    if not Path(path).is_dir():\n",
    "        print(f\"Path {path} is either not a directory, or does not exist.\")\n",
    "        exit()\n",
    "\n",
    "    indices = next(iter(np.load(f\"{path}/scene_info.npy\", allow_pickle=True).item().values()))\n",
    "    indices = list(range(indices[0], indices[1] + 1))\n",
    "\n",
    "    scene_info = np.load(f\"{path}/scene_info.npy\", allow_pickle=True)\n",
    "    print(scene_info)\n",
    "\n",
    "    annotations = np.load(f\"{path}/lang_annotations/auto_lang_ann.npy\", allow_pickle=True).item()\n",
    "    annotations = list(zip(annotations[\"info\"][\"indx\"], annotations[\"language\"][\"ann\"]))\n",
    "    print(annotations)\n",
    "    print(len(annotations))\n",
    "\n",
    "    idx = 0\n",
    "    # idx = 60000\n",
    "    ann_idx = -1\n",
    "\n",
    "    while True:\n",
    "        t = np.load(f\"{path}/episode_{indices[idx]:07d}.npz\", allow_pickle=True)\n",
    "\n",
    "        for d in data:\n",
    "            if d not in t:\n",
    "                print(f\"Data {d} cannot be found in transition\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.resize(t[d], (400, 400))\n",
    "            cv2.imshow(d, img[:, :, ::-1])\n",
    "\n",
    "        for n, ((low, high), ann) in enumerate(annotations):\n",
    "            if indices[idx] >= low and indices[idx] <= high:\n",
    "                if n != ann_idx:\n",
    "                    print(f\"{ann}\")\n",
    "                    ann_idx = n\n",
    "\n",
    "        # user_input = input(\"Enter something: \")\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(0)\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        elif key == 83:  # Right arrow\n",
    "            idx = (idx + 1) % len(indices)\n",
    "        elif key == 81:  # Left arrow\n",
    "            idx = (len(indices) + idx - 1) % len(indices)\n",
    "        else:\n",
    "            print(f'Unrecognized keycode \"{key}\"')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calvin_scene_D': [358482, 361252]}\n",
      "[((358656, 358720), 'move the light switch to turn on the yellow light'), ((359714, 359757), 'sweep the pink block to the right'), ((360978, 361011), 'place the block in the sliding cabinet'), ((358728, 358792), 'pick up the red block from the table'), ((359534, 359598), 'in the slider grasp the blue block'), ((360571, 360635), 'slide down the switch'), ((358729, 358793), 'pick up the red block on the table'), ((359069, 359103), 'place in slider'), ((360575, 360639), 'turn off the light bulb')]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/tim/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actions', 'rel_actions', 'robot_obs', 'scene_obs', 'rgb_static', 'rgb_gripper', 'rgb_tactile', 'depth_static', 'depth_gripper', 'depth_tactile']\n",
      "(7,)\n",
      "(7,)\n",
      "(200, 200, 3)\n",
      "(84, 84, 3)\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.07909500e-15\n",
      "  0.00000000e+00  0.00000000e+00  1.63225568e-01 -4.65878297e-02\n",
      "  4.59990009e-01  3.30461182e-07 -6.46899737e-08 -6.43902616e-01\n",
      " -1.63540040e-01  5.52660776e-02  4.60989636e-01  4.61108288e-05\n",
      " -7.81370023e-06 -2.05803878e+00 -2.77312162e-01  8.50804019e-02\n",
      "  4.60989884e-01 -2.63198658e-06  6.74378838e-06 -7.66156532e-01]\n"
     ]
    }
   ],
   "source": [
    "datapath_test= '.././test_data/D_D/task_D_D_episode.npz'\n",
    "data = np.load(datapath_test)\n",
    "print(list(data.keys()))\n",
    "print(data['actions'].shape)\n",
    "print(data['rel_actions'].shape)\n",
    "print(data['rgb_static'].shape)\n",
    "print(data['rgb_gripper'].shape)\n",
    "print(data['scene_obs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning\n",
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "max_seq_length = 16\n",
    "\n",
    "path = '/home/tim/calvin_debug_dataset/'\n",
    "path_parsed = '/home/tim/calvin_debug_dataset_parsed/'\n",
    "\n",
    "datapath_dd_training = '{}training'.format(path_parsed)\n",
    "datapath_dd_val = '{}validation'.format(path)\n",
    "caption_path_training = '{}training/lang_annotations/auto_lang_ann.npy'.format(path_parsed)\n",
    "caption_path_val = '{}validation/lang_annotations/auto_lang_ann.npy'.format(path)\n",
    "\n",
    "# train_dataset = CustomDataset(datapath_dd_training, caption_path_training, tokenizer, max_seq_length)\n",
    "train_dataset = CustomDataset(datapath_dd_training, caption_path_training, tokenizer, max_seq_length)\n",
    "val_dataset  = CustomDataset(datapath_dd_val, caption_path_val, tokenizer, max_seq_length)\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 14:06:17.757643: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:17.799950: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:17.800449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 14:06:18.593898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-28 14:06:21.440438: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:21.480348: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:21.480717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 14:06:22.237307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-28 14:06:25.186724: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:25.237445: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:25.238133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 14:06:26.024954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-28 14:06:29.212770: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:29.260706: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:06:29.261388: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 14:06:30.087594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tim/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tim/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tim/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tim/hulc_captioning/captioning/dataset.py\", line 119, in __getitem__\n    actions[j] = torch.tensor(data['actions'])  # Assign values to the tensor\n  File \"/home/tim/.local/lib/python3.10/site-packages/numpy/lib/npyio.py\", line 260, in __getitem__\n    raise KeyError(\"%s is not a file in the archive\" % key)\nKeyError: 'actions is not a file in the archive'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor batch in train_dataloader:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m   # print(\"token: \", batch.token.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m    break\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m     19\u001b[0m    \u001b[39m# print(\"token: \", batch.token.shape)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgpt_tokens: \u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m.\u001b[39mgpt_tokens\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgpt_mask: \u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m.\u001b[39mgpt_mask\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tim/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tim/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tim/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tim/hulc_captioning/captioning/dataset.py\", line 119, in __getitem__\n    actions[j] = torch.tensor(data['actions'])  # Assign values to the tensor\n  File \"/home/tim/.local/lib/python3.10/site-packages/numpy/lib/npyio.py\", line 260, in __getitem__\n    raise KeyError(\"%s is not a file in the archive\" % key)\nKeyError: 'actions is not a file in the archive'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens.shape)\n",
    "    print(\"gpt_mask: \", batch.gpt_mask.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens[0])\n",
    "    print(\"gpt_mask: \", batch.gpt_mask[0])\n",
    "   # if(torch.all(torch.eq(batch.gpt_tokens, batch.token))):\n",
    "   #     print(\"correct\")\n",
    "    print(batch.instruction[0])\n",
    "    print(\"actions: \", batch.actions.shape)\n",
    "    print(\"rgb_static: \", batch.rgb_static.shape)\n",
    "    print(\"rgb_gripper: \", batch.rgb_gripper.shape)\n",
    "    print(\"#############################################\")\n",
    "    break\n",
    "\"\"\"\n",
    "\n",
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens.shape)\n",
    "    print(\"gpt_mask: \", batch.gpt_mask.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens[0])\n",
    "    print(\"gpt_mask: \", batch.gpt_mask[0])\n",
    "   # if(torch.all(torch.eq(batch.gpt_tokens, batch.token))):\n",
    "   #     print(\"correct\")\n",
    "    print(batch.instruction[0])\n",
    "    print(\"actions: \", batch.actions.shape)\n",
    "    print(\"observations: \", batch.observations.shape)\n",
    "    # print(\"observations: \", batch.observations)\n",
    "    print(\"#############################################\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Captioning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find sutiable caption for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place the block in the sliding cabinet :  1\n",
      "place in slider :  1\n",
      "slide down the switch :  1\n",
      "pick up the red block on the table :  1\n",
      "in the slider grasp the blue block :  1\n",
      "turn off the light bulb :  1\n",
      "pick up the red block from the table :  1\n",
      "sweep the pink block to the right :  1\n",
      "move the light switch to turn on the yellow light :  1\n"
     ]
    }
   ],
   "source": [
    "annotations = np.load(f\"/home/tim/calvin_debug_dataset_parsed/training/lang_annotations/auto_lang_ann.npy\", allow_pickle=True).item()\n",
    "annotations = annotations[\"language\"][\"ann\"]\n",
    "\n",
    "unique_annotaions = set(annotations)\n",
    "for unique_annotaion in unique_annotaions:\n",
    "    print(unique_annotaion, ': ', annotations.count(unique_annotaion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MappingType(Enum):\n",
    "    MLP = 'mlp'\n",
    "    Transformer = 'transformer'\n",
    "\n",
    "class BehaviourEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        # self.pos_encoder = PositionalEncoding(CFG.d_model, dropout=CFG.dropout)\n",
    "        self.pos_encoder = PositionalEncoding(2050, dropout=0.1)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=CFG.d_model, nhead=CFG.n_heads, batch_first=True, dim_feedforward=CFG.d_ff, dropout=CFG.dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=CFG.n_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        image_features = src.observations # images: [batch_size, sequence_length, 2048]\n",
    "        actions = src.actions\n",
    "\n",
    "        src_key_padding_mask = (image_features.mean(dim=2)==0.0)\n",
    "        features = torch.cat((image_features, actions), dim=-1)\n",
    "\n",
    "        # Transformer encoder\n",
    "        # add positional encoding\n",
    "        features = features * math.sqrt(CFG.d_model)mp.set_start_method('spawn')\n",
    "        features = self.pos_encoder(features)\n",
    "        behaviour_encoding = self.transformer_encoder(features, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return behaviour_encoding\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
    "                                                                     clip_length, num_layers)\n",
    "        self.behaviour_encoder = BehaviourEncoder()\n",
    "        self.project_to_gpt = nn.Linear(514, self.gpt_embedding_size).to(\"cuda\")\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, data: AttrDict):\n",
    "\n",
    "        tokens = data.gpt_tokens\n",
    "        gpt_mask = data.gpt_mask\n",
    "        labels = None\n",
    "\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        behaviour_encoding  = self.behaviour_encoder(data)\n",
    "        \n",
    "        # !!!!!!!!!!!\n",
    "        behaviour_encoder_padding_mask = ~(data.rgb_static.mean(dim=2)==0.0) * 1.0\n",
    "\n",
    "        prefix_projections = self.project_to_gpt(behaviour_encoding)\n",
    "\n",
    "        total_mask = torch.cat((behaviour_encoder_padding_mask, embedding_text), dim=1)\n",
    "\n",
    "        ######################################\n",
    "        # prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=total_mask)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
