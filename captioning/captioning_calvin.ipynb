{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/tim/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-29 14:18:14.612762: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 14:18:14.749092: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 14:18:14.750592: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 14:18:16.272813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from r3m import load_r3m\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
    "from torch.nn import functional as nnf\n",
    "import wandb\n",
    "\n",
    "from dataset import CustomDataset, AttrDict \n",
    "\n",
    "import config as CFG\n",
    "\n",
    "\n",
    "mp.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Calvin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "\n",
    "    #parser = ArgumentParser(description=\"Interactive visualization of CALVIN dataset\")\n",
    "    #parser.add_argument(\"path\", type=str, help=\"Path to dir containing scene_info.npy\")\n",
    "    #parser.add_argument(\"-d\", \"--data\", nargs=\"*\", default=[\"rgb_static\", \"rgb_gripper\"], help=\"Data to visualize\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    path = CFG.datapath_training\n",
    "    data = [\"rgb_static\", \"rgb_gripper\"]\n",
    "\n",
    "    if not Path(path).is_dir():\n",
    "        print(f\"Path {path} is either not a directory, or does not exist.\")\n",
    "        exit()\n",
    "\n",
    "    indices = next(iter(np.load(f\"{path}/scene_info.npy\", allow_pickle=True).item().values()))\n",
    "    indices = list(range(indices[0], indices[1] + 1))\n",
    "\n",
    "    scene_info = np.load(f\"{path}/scene_info.npy\", allow_pickle=True)\n",
    "    print(scene_info)\n",
    "\n",
    "    annotations = np.load(f\"{path}/lang_annotations/auto_lang_ann.npy\", allow_pickle=True).item()\n",
    "    annotations = list(zip(annotations[\"info\"][\"indx\"], annotations[\"language\"][\"ann\"]))\n",
    "    print(annotations)\n",
    "    print(len(annotations))\n",
    "\n",
    "    idx = 0\n",
    "    # idx = 60000\n",
    "    ann_idx = -1\n",
    "\n",
    "    while True:\n",
    "        t = np.load(f\"{path}/episode_{indices[idx]:07d}.npz\", allow_pickle=True)\n",
    "\n",
    "        for d in data:\n",
    "            if d not in t:\n",
    "                print(f\"Data {d} cannot be found in transition\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.resize(t[d], (400, 400))\n",
    "            cv2.imshow(d, img[:, :, ::-1])\n",
    "\n",
    "        for n, ((low, high), ann) in enumerate(annotations):\n",
    "            if indices[idx] >= low and indices[idx] <= high:\n",
    "                if n != ann_idx:\n",
    "                    print(f\"{ann}\")\n",
    "                    ann_idx = n\n",
    "\n",
    "        # user_input = input(\"Enter something: \")\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(0)\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        elif key == 83:  # Right arrow\n",
    "            idx = (idx + 1) % len(indices)\n",
    "        elif key == 81:  # Left arrow\n",
    "            idx = (len(indices) + idx - 1) % len(indices)\n",
    "        else:\n",
    "            print(f'Unrecognized keycode \"{key}\"')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calvin_scene_D': [358482, 361252]}\n",
      "[((358656, 358720), 'move the light switch to turn on the yellow light'), ((359714, 359757), 'sweep the pink block to the right'), ((360978, 361011), 'place the block in the sliding cabinet'), ((358728, 358792), 'pick up the red block from the table'), ((359534, 359598), 'in the slider grasp the blue block'), ((360571, 360635), 'slide down the switch'), ((358729, 358793), 'pick up the red block on the table'), ((359069, 359103), 'place in slider'), ((360575, 360639), 'turn off the light bulb')]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/tim/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actions', 'rel_actions', 'robot_obs', 'scene_obs', 'rgb_static', 'rgb_gripper', 'rgb_tactile', 'depth_static', 'depth_gripper', 'depth_tactile']\n",
      "(7,)\n",
      "(7,)\n",
      "(200, 200, 3)\n",
      "(84, 84, 3)\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.07909500e-15\n",
      "  0.00000000e+00  0.00000000e+00  1.63225568e-01 -4.65878297e-02\n",
      "  4.59990009e-01  3.30461182e-07 -6.46899737e-08 -6.43902616e-01\n",
      " -1.63540040e-01  5.52660776e-02  4.60989636e-01  4.61108288e-05\n",
      " -7.81370023e-06 -2.05803878e+00 -2.77312162e-01  8.50804019e-02\n",
      "  4.60989884e-01 -2.63198658e-06  6.74378838e-06 -7.66156532e-01]\n"
     ]
    }
   ],
   "source": [
    "datapath_test= '.././test_data/D_D/task_D_D_episode.npz'\n",
    "data = np.load(datapath_test)\n",
    "print(list(data.keys()))\n",
    "print(data['actions'].shape)\n",
    "print(data['rel_actions'].shape)\n",
    "print(data['rgb_static'].shape)\n",
    "print(data['rgb_gripper'].shape)\n",
    "print(data['scene_obs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Captioning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MappingType(Enum):\n",
    "    MLP = 'mlp'\n",
    "    Transformer = 'transformer'\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        # pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class BehaviourEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.pos_encoder = PositionalEncoding(CFG.d_model, dropout=CFG.dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=CFG.d_model, nhead=CFG.n_heads, batch_first=True, dim_feedforward=CFG.d_ff, dropout=CFG.dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=CFG.n_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        image_features = src.observations # images: [batch_size, sequence_length, 2048]\n",
    "        actions = src.actions\n",
    "\n",
    "        src_key_padding_mask = (image_features.mean(dim=2)==0.0)\n",
    "        features = torch.cat((image_features, actions), dim=-1)\n",
    "\n",
    "        # Transformer encoder\n",
    "        # add positional encoding\n",
    "        features = features * math.sqrt(CFG.d_model)\n",
    "        features = self.pos_encoder(features)\n",
    "        behaviour_encoding = self.transformer_encoder(features, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return behaviour_encoding\n",
    "\n",
    "class TransformerMapper(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
    "        super(TransformerMapper, self).__init__()\n",
    "        self.clip_length = clip_length\n",
    "        #self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=8, batch_first=True, dim_feedforward=int(dim_embedding * 2), dropout=0.0)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
    "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
    "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
    "        prefix = torch.cat((x, prefix), dim=1)\n",
    "        out = self.transformer(prefix)[:, self.clip_length:]\n",
    "        return out\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
    "                                                                     clip_length, num_layers)\n",
    "        self.behaviour_encoder = BehaviourEncoder()\n",
    "        self.project_to_gpt = nn.Linear(514, self.gpt_embedding_size).to(\"cuda\")\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, data: AttrDict):\n",
    "\n",
    "        tokens = data.gpt_tokens\n",
    "        gpt_mask = data.gpt_mask\n",
    "        labels = None\n",
    "\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        behaviour_encoding  = self.behaviour_encoder(data)\n",
    "        \n",
    "        behaviour_encoder_padding_mask = ~(data.observations.mean(dim=2)==0.0) * 1.0\n",
    "        prefix_projections = self.project_to_gpt(behaviour_encoding)\n",
    "\n",
    "        total_mask = torch.cat((behaviour_encoder_padding_mask, embedding_text), dim=1)\n",
    "\n",
    "        # prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=total_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find sutiable caption for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place in slider :  1\n",
      "turn off the light bulb :  1\n",
      "place the block in the sliding cabinet :  1\n",
      "in the slider grasp the blue block :  1\n",
      "pick up the red block from the table :  1\n",
      "slide down the switch :  1\n",
      "move the light switch to turn on the yellow light :  1\n",
      "sweep the pink block to the right :  1\n",
      "pick up the red block on the table :  1\n"
     ]
    }
   ],
   "source": [
    "annotations = np.load(f\"/home/tim/calvin_debug_dataset_parsed/training/lang_annotations/auto_lang_ann.npy\", allow_pickle=True).item()\n",
    "annotations = annotations[\"language\"][\"ann\"]\n",
    "\n",
    "unique_annotaions = set(annotations)\n",
    "for unique_annotaion in unique_annotaions:\n",
    "    print(unique_annotaion, ': ', annotations.count(unique_annotaion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "datapath_training_parsed = CFG.datapath_training_parsed\n",
    "datapath_val = CFG.datapath_val_parsed\n",
    "caption_path_training = '{}/lang_annotations/auto_lang_ann.npy'.format(datapath_training_parsed)\n",
    "caption_path_val = '{}/lang_annotations/auto_lang_ann.npy'.format(datapath_val)\n",
    "\n",
    "# train_dataset = CustomDataset(datapath_dd_training, caption_path_training, tokenizer, max_seq_length)\n",
    "train_dataset = CustomDataset(datapath_training_parsed, caption_path_training, tokenizer, CFG.max_seq_length)\n",
    "val_dataset  = CustomDataset(datapath_val, caption_path_val, tokenizer, CFG.max_seq_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 11:22:51.012691: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:22:51.067767: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:22:51.068960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 11:22:51.866908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-29 11:22:54.822551: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:22:54.862819: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:22:54.863226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 11:22:55.658382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-29 11:22:58.563532: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:22:58.603776: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:22:58.604162: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 11:22:59.383665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-29 11:23:02.737962: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:23:02.778428: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-29 11:23:02.778821: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 11:23:03.762031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_tokens:  torch.Size([9, 16])\n",
      "gpt_mask:  torch.Size([9, 16])\n",
      "gpt_tokens:  tensor([27729,   510,   262,  2266,  2512,   319,   262,  3084,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "gpt_mask:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "pick up the red block on the table\n",
      "actions:  torch.Size([9, 64, 7])\n",
      "observations:  torch.Size([9, 6, 2048])\n",
      "#############################################\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens.shape)\n",
    "    print(\"gpt_mask: \", batch.gpt_mask.shape)\n",
    "    print(\"gpt_tokens: \", batch.gpt_tokens[0])\n",
    "    print(\"gpt_mask: \", batch.gpt_mask[0])\n",
    "   # if(torch.all(torch.eq(batch.gpt_tokens, batch.token))):\n",
    "   #     print(\"correct\")\n",
    "    print(batch.instruction[0])\n",
    "    print(\"actions: \", batch.actions.shape)\n",
    "    print(\"observations: \", batch.observations.shape)\n",
    "    # print(\"observations: \", batch.observations)\n",
    "    print(\"#############################################\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"clipcalvin\")\n",
    "\n",
    "def validate(model: ClipCaptionModel):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_dataloader):\n",
    "            data.observations = data.observations.to(CFG.device)\n",
    "            data.actions = data.actions.to(CFG.device)\n",
    "            # data.instruction = data.instruction.to(CFG.device)\n",
    "            data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "            data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            logits = outputs.logits[:, data.observations.shape[1] - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), data.gpt_tokens.flatten(), ignore_index=0)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_dataloader)\n",
    "\n",
    "def train(model: ClipCaptionModel,\n",
    "          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    epochs = CFG.epochs\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            data.observations = data.observations.to(CFG.device)\n",
    "            data.actions = data.actions.to(CFG.device)\n",
    "            # data.instruction = data.instruction.to(CFG.device)\n",
    "            data.gpt_tokens = data.gpt_tokens.to(CFG.device)\n",
    "            data.gpt_mask = data.gpt_mask.to(CFG.device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            logits = outputs.logits[:, data.observations.shape[1] - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), data.gpt_tokens.flatten(), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            progress.update()\n",
    "            if (idx+1) % 2000 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "                val_loss = validate(model)\n",
    "                wandb.log({\"val_loss\": val_loss})\n",
    "        progress.close()\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "   # print(\"token: \", batch.token.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
